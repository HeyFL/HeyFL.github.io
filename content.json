{"posts":[{"title":"","text":"蔡振谦的个人简历 联系方式 手机：15625290660 Email：361389383@qq.com.com QQ/微信号：361389383 博客1(主): https://heyfl.github.io 博客2(备): https://heyfl.gitee.io GitHub: https://github.com/HeyFL 个人信息 蔡振谦/软件工程本科/男 工作年限：8年 期望职位：Java开发/Leader 期望行业: 互联网/金融 期望城市：深圳 语言能力: 粤语&amp;普通话 other: CET4&amp;在校4年奖学金 自我描述 本人目前就职于顺丰科技有限公司,目前在团队中进行公司几套核心系统研发工作,主要优势: 有高并发、大数据量的大型分布式系统网站丰富的研发、设计经验 有较好的跨部门、跨组织协调能力 有Leader经验 曾为部门预备架构师有较好的架构设计能力 工作经历 顺丰科技有限公司(正编) （ 2019年11月 ~ ） 任职于顺丰科技有限公司核心团队-OMS研发部，该部门主要负责公司物流数据中台相关服务，提供公司数十亿级订/运单承载、存储、分发、实时查询等一系列科技核心服务； 项目为基于SFDubbo(Spring+dubbo)与SFOpen框架搭建的服务体系项目，服务之间通过Kfk、http、dubbo通讯，核心查询服务QPS&gt;3w； 在该部门主要负责中台系统的需求分析、设计开发、重构、优化，并选择合理的方案实现核心难题 为全科技11套核心系统中的2套系统进行了重构优化工作，包含并不仅限于: 查单性能提高90%(ES查询耗时降低90%,ES查询量降低99.8%;ES内存负载降低85.7%,CPU降低50%) OMS数据库数据倾斜解决(由根据城市生成分库号,改成完全随机) Redis缓存压缩减少70%存储(且存取耗时有一定的提升,21年双十一节约14.5T内存) 合理使用配置熔断降级(提高外部系统无响应时的系统可用性) Hbase region数据倾斜导致的超时问题(设定新RowKey规则) Hbase数据压缩,解决Region数过多,占用存储过多问题(新数据region分区数减少越80%至3000+) 防止暴力调用设计（防爬） 深圳市递四方信息科技有限公司 （ 2017年09月 ~ 2019年9月 ） 递四方信息科技有限公司(阿里系,跨境物流),目前团队中作为TeamLeader/预备架构师 项目为基于SpringCloud框架微的服务体系项目，通过Jenkins构建发布到对应服务器，服务之间通过MQ、http、dubbo通讯； 在该部门主要负责GRID产品线/G2G产品线的需求分析与设计，并选择合理的方案实现核心难题。 期间,自发为部门开发共用模块/优化,包含但不仅限如下: 1. 基础依赖: 1. 统一的业务异常/系统异常 2. Http统一的返回对象 2. Redis分布式锁工具类 3. 基于Redis的幂等注解 4. 消息级延迟补偿 5. MQ预警&amp;手动迁移服务 6. 统一异常处理(统一解决了异常日志输出及返回问题) 7. 统一国际化工具类(减少了团队对[前端]请求的后端国际化的工作量) 其他 定义异常处理/抛出规范 定义Consul配置规范(减少发版比对时的困难) 定义RabbitMQ在SpringBoot下的队列定义规范 北京思特奇信息技术股份有限公司 （ 2015年04月 ~ 2017年08月 ） 技能清单 以下均为我使用的技能 精通Java、熟悉JVM： 熟悉各种集合使用及其原理，熟悉jvm内存结构、java内存模型，熟悉各种垃圾回收算法与回收器并可以适当进行调优，通读《深入理解Java虚拟机》 熟悉常见数据结构与算法： 熟悉常见数据结构、算法的优缺点及其使用场景 精通Spring： 通读getBean、AOP、事务、MVC、整合Mybatis等源码，理解原理，并能进行二次开发、扩展 熟悉常见高可用上线方案 熟练使用灰度发布方案，并有较多的实践经验 熟悉常见SpringCloud等微服务框架： 熟悉熔断、降级、Feign等使用 熟悉使用及了解常见MQ ，Redis原理： RabbitMQ，消息路由，延迟队列，独占队列，事务消息，消息不丢，顺序消费等 Kafka的分区、消费、 熟练使用常见Redis数据结构，理解Redis线程模型，熟悉缓存一致性等方案 熟悉MySQL： 理解MVCC，事务及其实现原理 熟悉Zookeeper/dubbo 熟悉SVN/Git/Jenkins 理解分布式事务 理解常见大数据技术并有一定的使用与优化经验 使用Hbase、ElasticSearch、Flink等并有一定的理解，同时对Hbase与ES的项目应用有过优化经验 致谢 感谢您花时间阅读我的简历，期待能有机会和您共事。 附录 对应项目经验 OMS产品线(订/运单) [1.5年] OMS为公司的订单/运单业务中台系统，承接B类客户（BSP）与C类客户（CX）的所有订单，以及其他所有大网相关的运单数据，提供补充、分发、实时查询服务 包括以下系统 OMS订单服务（主） OMS运单服务 订单、运单查询服务 个人在项目职责： 系统存储、性能优化、系统升级设计 其核心业务模块(订单/运单、查询服务)的详细设计/开发/对接 系统故障分析排查处理 提高系统容错性可用性 BSP产品线(协助重构) [0.5年] BSP为公司主要的订单以及收入来源, 该系统为公司的核心系统, 但是由于历史原因, 系统的可维护性和可扩展性较差, 该系统的重构工作主要是为了提高系统的可维护性和可扩展性, 以便于后续的业务扩展和系统升级 协助重构BSP系统，主要协助负责梳理BSP系统的业务逻辑，对BSP系统进行重构，提高系统的可维护性和可扩展性 项目重构目的: Oracle转Mysql 框架升级为SF微服务框架(SFOpen) 删除冗余代码、删除0流量的业务 系统性能优化 SISP产品线 [1年] SISP产品线原为公司面向客服、仓管人员的包裹信息查询系统，为期提供一站式服务。后扩展承接OMS产品线的订运单查询服务后，为公司内部提供一系列实时的订、运单查询服务。 个人在项目负责： 以动静分离+业务切分部署的方式升级重构老的单体SISP服务 系统故障分析排查处理 日常业务开发、多部门新业务模块协调开发 提高系统容错性可用性 G2G产品线(启运仓相关) 项目描述 跨国部署的微服务体系，含2个业务项目 : G2G仓内运营调度系统 主要是解析处理调度系统下发的任务单，拆分为对应的作业单下发给下游的作业系统并反馈操作结果 G2G仓内作业系统 主要是对上游下发的作业单进行作业，并且反馈操作结果 个人在项目职责(Leader)： 系统的架构设计 其核心模块(预报/调度分发/结果回传/数据权限等)的详细设计/开发 GRID产品线(包括接单/调度/运营/门店作业部分) 项目描述 跨国部署的微服务体系，含4个业务项目（主要） : 接单平台 调度中心 运营中心 作业系统（提供门店PC端，派送揽收APP端服务） 个人在项目职责： 负责核心模块(下单/其他接单系统与客户系统对接的预报/共用模块) [参与]系统的架构设计 联通业务支撑平台 项目描述 联通内部员工使用的交尾正统的分布式业务受理系统，包括：业务受理、IT 需求、调账管理、销售支撑、产品管理、系统管理、经营分析等；技术体系包含: 大概技术:Nginx+Keepalive+zookeeper+工作流 +共享session单点登录+类似SpringMVC+Spring+Ibatis/Mybatis+Maven 个人在项目职责 业支-调账管理模块: 分析设计开发、产品管理模块的维护 业支系统前台整体架构 与 后台整体架构维护、改造","link":"/about/about.html"},{"title":"分布式事务解法","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Distributed/How-To-Use-Distributed-Transaction.html 可搭配参考我的GitHub: 分布式事务.xmind 参见分布式事务有哪些 1. 基于XA协议的全局事务 (强一致性) 使用情况：一个工程对多个数据源（数据库需要支持XA协议） 基本功能：数据库时间功能+开源组件（知名的分布式事务管理器主要有atomikos、bitronix、narayana。其中，仅atomikos支持XA和TCC两种机制，bitronix、narayana仅支持XA机制。） 原理：2PC/3PC 单服务多数据库使用全局事务 缺点：XA协议比较简单，成本较低，但是其单点问题，以及不能支持高并发(由于同步阻塞)依然是其最大的弱点(主要),2PC本身的缺陷也是一方面原因(次要)。 2PC &amp; 3PC 2PC是两阶段提交协议（Two-Phase Commit） 它保证了所有参与节点在一个事务中要么全部提交，要么全部回滚。2PC有很高的可靠性和一致性，但需要进行两次网络通信和等待，从而导致性能上的一定损失。2PC适用于对事务强一致性要求高的系统，例如银行转账等 3PC是三阶段提交协议（Three-Phase Commit） 它在2PC的基础上增加了一个阶段，以减少等待时间和网络开销。3PC需要的网络通信比2PC更少，因此在性能方面略优于2PC。3PC适用于对事务一致性要求高，但是对性能有一定要求的系统，例如电子商务、在线游戏等 2PC &amp; 3PC区别 2PC: 只有协调者有超时机制，超时后，发送回滚指令 3PC：协调者和参与者都有超时机制。 协调者超时：发送中断指令。 参与者超时：pre阶段进行中断，do阶段进行提交 2PC &amp; 3PC使用场景 2PC的使用场景：适用于数据量不大，事务并发度不高的场景，例如金融交易、订单操作等 3PC的使用场景：适用于数据量较大，分布式节点数量较多，对性能和可靠性要求较高的场景，例如：例如电子商务、在线游戏等 2. TCC(强一致性) 使用情况：多系统 多数据源 原理：编程式事务，每个业务都要开发try，confirm，cancel 3个方法实现 try-&gt;调用所有服务,把所有资源设置为中间状态(如订单设置为支付中,库存设置为冻结) confirm-&gt;把所有资源状态设置为完成(支付完成) cancel-&gt; 把所有资源状态回滚回try前 缺点：程序复杂度高，confirm、cancel方法需要实现幂等 3. 基于可靠消息服务的分布式事务–MQ消息队列(最终一致性) 使用情况： 多系统 多数据源 原理：利用本地事务及MQ的事务消息（其实也不一定要用），确保本地事务未完成、消息发送出去；理想地认为对方一定能正确、成功消费消息； 缺点： 数据有一小段时间不一致 对方系统出现问题，不能正常消费信息，导致数据长期不一致 4. (最大努力通知)基于[不可靠]消息服务的分布式事务–MQ消息队列(最终一致性) 使用情况： 多系统 多数据源 原理：通过MQ延迟队列等实现的延迟通知服务, 通知服务会每隔1,5,10,30分钟重复通知 , 到达最大通知次数后, 需要人工通知或重置重试次数 优势(比基于可靠消息服务的分布式事务) 允许消费方短期异常(通过1,5,10,30分钟的间隔重试) 缺点： 数据有一小段时间不一致 对方系统出现问题，不能正常消费信息，导致数据长期不一致 编码比基于可靠消息服务的分布式事务复杂一些,需要实现延迟队列、自产自销+Http请求等实现重复通知 分布式事务选型 分布式事务选型主要要看场景，我们以流量充值作为例子 流量充值涉及到订单支付 金钱交易严格用tcc 订单支付完后要给用户增加积分 这种情况这个必要成功（毕竟是内部系统），用最终消息一致性方案就行了; 订单支付完后还要给用户发送一条短信 短信一般是跟电信运营商的第三方接口对接，有可能成功有可能失败，用最大努力通知方案(每隔1,5,10,30分钟重复通知 直到达到最大重试次数)","link":"/Distributed/How-To-Use-Distributed-Transaction.html"},{"title":"生产故障分析：线程池配置错误导致的阻塞问题","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Bug-Log-Optimization/thread-pool-use-error.html 在生产环境中，我们遇到了一个由线程池配置错误导致的阻塞问题。本文将对这个问题进行详细分析，并提出相应的解决方案。 问题背景 后端在接收到前端用户请求后，会将请求分成4个线程交给一个线程池处理。这4个线程分别负责请求不同的查询接口，以获取查询结果。最后，将这些结果合并并同步返回给前端用户。 12345678910111213141516// 提交任务到线程池waybillNoQueryFuture = submitTask(waybillNoQueryService, waybillNo, currentUserName)orderInfoQueryFuture = submitTask(orderInfoQueryService, waybillNo, currentUserName)appointmentQueryFuture = submitTask(appointmentQueryService, waybillNo, currentUserName)operationWaybillQueryFuture = submitTask(operationWaybillQueryService, waybillNo, currentUserName)// 获取任务结果waybillNoDto = waybillNoQueryFuture.getResult()orderInfoDto = orderInfoQueryFuture.getResult()appointmentDto = appointmentQueryFuture.getResult()operationWaybillDto = operationWaybillQueryFuture.getResult()// 合并结果并返回给前端用户result = mergeResults(waybillNoDto, orderInfoDto, appointmentDto, operationWaybillDto)return result 相关线程池基本原理 Java线程池（ThreadPoolExecutor）是一种基于线程的Executor框架，它主要用于管理并行执行的任务。线程池中的线程会被复用，从而降低了线程创建和销毁的开销 线程池的主要组成部分包括： 核心线程数（corePoolSize）：线程池中始终保持的线程数量 最大线程数（maximumPoolSize）：线程池中允许的最大线程数量 工作队列（BlockingQueue）：用于存放待处理任务的队列当线程池中的线程数量达到核心线程数时，新的任务会被放入工作队列中等待执行 拒绝策略（RejectedExecutionHandler）：当线程池中的线程数量达到最大值，并且工作队列已满时，新的任务会触发拒绝策略 线程池队列的作用是在线程池中的线程数达到核心线程数时，将新的任务暂存起来，等待空闲线程来处理 如果工作队列长度设置得过短，当线程池中的线程数达到核心线程数，并且工作队列已满时，新的任务将触发拒绝策略 不同的拒绝策略会有不同的处理方式，例如抛出异常、丢弃任务等 问题分析 线程池配置错误 在这个案例中，线程池的配置出现了错误。线程池的等待队列长度被错误地设置为1： 1234corePoolSize = 10; maxPoolSize = 20; queueCapacity = 1;createThreadPool(corePoolSize, maxPoolSize, queueCapacity, rejectionPolicy); 由于线程池等待队列长度设置过小，每当前端发来一个请求，同时向线程池提交4个任务时，就会超过队列长度。这导致触发了线程池的拒绝策略。在这个案例中，拒绝策略的设置是什么都不做。 1234rejectionPolicy = (task, executor) -&gt; { // 什么都不做，丢弃任务，记录日志 log.error(&quot;Task rejected&quot;); }; } 因此，主线程无法获得任务执行结果，从而导致一直阻塞。 解决方案 方案1 为了解决这个问题，可以考虑调整线程池的配置，将等待队列长度设置为一个合理的值，以避免触发拒绝策略 12// 调整队列长度 queueCapacity = 50; // 或 100 方案2 修改拒绝策略 1234567rejectionPolicy = (task, executor) -&gt; { // 线程池繁忙，记录日志，提交让主线程处理 logger.info(&quot;thread pool is busy-&gt;{},{}&quot;, executor.getPoolSize(), executor.getQueue().size()); if (!executor.isShutdown()) { task.run(); }} 总结 这次分享了一个由线程池配置错误导致的阻塞问题，并给出了相应的解决方案。 在实际开发中，我们应当注意合理地配置线程池参数，并加强应用监控，以避免类似问题的发生。 理解线程池的原理和配置参数对于合理地利用线程池资源至关重要。在实际项目中，应该根据业务需求和系统资源来设置合适的线程池参数，以保证系统的稳定运行和良好性能。","link":"/Bug-Log-Optimization/thread-pool-use-error.html"},{"title":"Spring-Cloud服务在Consul中的异常注册","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Bug-Log-Optimization/bug_in_spring-cloud_instance_registered_with_consul.html 背景 公司实现微服务化并原来使用的Dubbo+Zookeeper实现应用间的服务调用，考虑到Dubbo不在维护最近想要切换为Spring Cloud+Consul 环境 Spring Cloud: Edgware.SR3 Spring-boot: 1.5.13.RELEASE 12345678910111213141516&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.13.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.SR3&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 根据网上流传的博客我们使用的配置为: 1spring.cloud.consul.discovery.instance-id=${spring.application.name}-${server.port} 也就是【服务名】+【端口】的形式来标识Consul上的一个服务 1. 第一个问题 已注册上Consul的服务被后启动的提供者覆盖(采用【服务名】+【端口】的形式) 在我们实际开发测试时候发现 无论服务起了多少个实例，最终展示到Consul都只有一个 : 怀疑是Consul以【服务名】+【端口】为实例的唯一标示，导致【后起来的服务】覆盖掉【原来已经注册到Consul上的服务】了 ，我们通过Feign调用时发现也确实是如此 1.1. Kill 第一个问题 上Spring官方文档轻松找到解决方案(有问题还是官方文档好) 1${spring.application.name}:${vcap.application.instance_id:${spring.application.instance_id:${random.value}}} 原理就是每次启动时注册的实例ID都为【服务名】+【随机数】： 通过这种形式，可以让每一个服务提供者（实例）都有效地注册到Consul上，并且Consul上的每一个Instance都能唯一映射到每一个提供者上 2. 第二、第三个问题 2.1. 服务器通过Kill -9的重启脚本快速重启导致一个提供者在Consul上注册了多次(Consul注册的实例数&gt;实际提供者数) 首先，我们要知道当系统执行kill-9命令的时候会立马强制关闭该进程，程序很可能正在处理请求中，同时也占用了一些的资源，本来需要做一些善后才能正常、安全的结束，但是你一个 kill-9命令过来，程序就措手不及了。 结合上述情况，实际上程序非正常重启，已经注册在consul上的服务没有被反注册，服务重新启动之后又重新的注册了一个新的服务上去了（而且重启后以前的服务在Consul心跳机制下海认为是可用的），一个服务就在Consul注册了多次了 这种情况可以修改重启脚本解决： 通过Kill -15直接关闭提供者进程 可以参考这篇文章 2.2. 【服务名】+【随机数】的InstanceID不能提供足够信息帮助我们快速定位问题 上面的方案可以解决我们遇到的问题，但是有一点不足的是，Consul上看到的InstanceID都是【服务名】+【随机数】，随机数没有可读性可言， 我们压根不能根据这个InstanceID一眼看出的它的服务提供者是谁。不便于我们排查问题 进一步优化 其实最好就是使用【服务名】+【机器IP】+【端口】的形式，这样既能通过唯一标示服务提供者解决 Consul服务被覆盖的问题 也能方便我们运维开发时快速知道当前服务的提供者列表，快速定位问题 最终方案 1spring.cloud.consul.discovery.instance-id=${spring.application.name}:${spring.cloud.client.ipAddress}:${server.port}","link":"/Bug-Log-Optimization/bug_in_spring-cloud_instance_registered_with_consul.html"},{"title":"数据库数据倾斜","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Bug-Log-Optimization/database-skew.html 其实改动不是很大，这里简单记录下 背景 OMS订单系统，日数据量较大平均近3kw/日,高峰达8kw+/日的订单需要存到数据库中（之前64库，现在扩到了128库）； 运维后台监控看到，各个库的压力不一，江浙沪，京津冀，深圳755等地区对应的数据库压力很高，而其他地区的数据库压力很低； 分析 数据库通过mycat，以分库号字段进行分库，以内部订单号分表 内部订单号规则： 3位分库号+系统来源（2位）+MMDDHHmmssSSS+订单类型+6位随机数+1位校验码=26位 分库号生成规则为根据地区、网点进行生成； 因此，可以看出，同一地区的订单，分库号是一样的，因此，同一地区的订单，会落在同一个库中； 解决 直接粗暴修改分库号生成规则为0~127区间随机生成，因为原有订单的删、改、查最终都会带上原有的内部订单号，所以不会影响到历史数据，只是新的订单会落在不同的库中； 上线方案 因为是S级系统，上线加了开关，如若有问题，可以快速回滚，因为是中台消费者系统，期间如果出问题的数据通过关闭开关+kfk重置偏移量解决。 上线后效果 （此处应有截图）","link":"/Bug-Log-Optimization/database-skew.html"},{"title":"线上ElasticJob堵塞问题排查","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Bug-Log-Optimization/elasticJob-bug-fix.html 背景 刚进新公司3天正逢双十一，领导生产补数出问题了，补数速度不稳定时快时慢，百万条数据补了好几个小时，之前都是几分钟搞定的，导致通了个宵； 早上我早到公司，领导截了个数据库的图给我，说交给我来看，然后刚通宵完的他就去开会汇报去了。。。我还没来得及熟悉环境，就被扔到了火坑里了，压力山大。。。 具体的过程没记录,这里只能凭回忆记录下了 问题描述 给我的截图大概长这样： 数据库分库号 需要补数的运单数量 分库0 12345 分库1 0 分库2 0 … *** 分库31 0 分库32 0 分库32 5w 分库33 4w 分库34 8w … *** 分库128 8w 知道的太少，跟同事了解业务,其实补数就是重推数据,把需要重推的单号记入表中,然后通过ElasticJob定时任务消费表中数据，实现重推数据 问题分析 没用过ElasticJob这类定时任务组件，先通过现象分析，发现分库32-128的数据都是好几w条，而且分库0-31的数据都是0，这就很奇怪了； 查看配置sharding-total-count=16，看现象猜测是只有一个分片在跑，且一轮跑16个分片，跑到第二轮； 问题分析 因为实在不熟悉定时任务组件逻辑，还是得去分析源码看原因才好解决问题； 看了下配置 12streaming-process=true sharding-total-count=16 streaming-process=true 代表流式处理； sharding-total-count=16 代表分片总数； 结合问题，猜测是只有一个分片在消费，且一轮跑16个分片，跑到第二轮； 查看流式计算源码发现流式计算逻辑其实是: 各个节点接收任务 切分16个分片,当前节点只消费一个分片 所有分片处理完后,分发触发下一(16个分片)次的任务 而我们系统对每个分片的处理实际上也是流式的,大致代码如下: 123while(fetchData()){ processData()} fetchData()每次获取200条数据，如果有数据就进行重推，直到结束； 结论 那么问题就很明显了，研发人员把重推任务设置为流式处理，但是每次重推只处理200条数据，导致每个分片处理完后，分发下一轮任务； 但是在第二轮任务因为这某个节点分片数据量很大，导致其他节点早就处理完了，而这个节点的数据迟迟处理不完，任务一直结束不了一直在等待这个分片的完成，消费效率滑坡； 处理方案 流式计算加入超时机制，每个分片最多只能处理一段时间，超过后就结束本次任务，分发下一轮任务 放弃流式计算，缩短定时任务间隔 这里推荐方案1，不过最终团队选择方案2，实现比较简单。。。","link":"/Bug-Log-Optimization/elasticJob-bug-fix.html"},{"title":"缓存密集加载导致数据库崩溃问题","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Bug-Log-Optimization/cache-load-database-crash.html 背景 SISP自己基本不存储业务数据，但是每个节点都需要本地缓存了一些网点、员工信息、月结用户信息等基础数据，生产监控发现数据库定期压力飙升，数据库CPU压力到达80+%如图： 用脚趾头分析 从图中可以看出，每天小时飙升一次，明显就是定时任务大批量查询数据库导致的，在SISP系统，也就只有加载缓存可能会导致 找到运维获取慢日志，发现大量的查询语句，如下： 12345SELECT DISTINCT nd.division_code AS city_code, nnd.dist_cn_name , nnd.dist_en_name FROM tm_new_district nd, tm_new_district nnd WHERE nd.division_code IN( SELECT DISTINCT t.CITY_CODE FROM tm_department t,tm_district d WHERE t.DIST_CODE = d.DIST_CODE AND t.DELETE_FLG = 0 AND d.DIST_NAME LIKE '%/%' )AND nnd.dist_code = nd.city_code 因为是单库，数据量也大，单条sql查询耗时近40s，因此，大量查询导致数据库压力飙升 解决 解决手段1：缓存刷新时间分散开，错峰加载缓存 因为是通用方法，这里8分钟内刷新一次的小任务就忽略了。我们对大于8分钟的任务，增加了随机时间间隔，如下代码所示（其中delayPercent = 0.2，表示随机减少时间定时任务0~20%的间隔时间）： 123456789101112131415161718192021/** * 将日期加上一个随机的时间间隔 * @param date 原始日期 * @return 计算后的新日期 */private Date scheduleTimeAddRandom(Date date) { // 获取当前时间戳 long now = System.currentTimeMillis(); // 计算初始时间间隔 long initialDelay = date.getTime() - now; // 获取当前线程的随机数生成器 ThreadLocalRandom random = ThreadLocalRandom.current(); // 对于间隔超过8分钟的任务，随机减少一部分时间间隔 if (initialDelay * delayPercent &gt; (8 * 60 * 1000 * delayPercent) &amp;&amp; initialDelay &gt; 0) { initialDelay = initialDelay - random.nextLong((long) (initialDelay * delayPercent)); } // 计算最终的下次执行时间 Date nextExecuteTm = new Date(now + initialDelay); return nextExecuteTm;} 解决手段2： 优化慢查询sql 重新梳理业务逻辑，其实我们只是需要城市代码对应的多城市名而已，其在tm_new_district中就有了，只是我们没有从上游上同步到我们数据库罢了。 同步完对应表字段后，完全不需要连接表，用一条简单sql就可以了，新的SQL： 1SELECT DIST_CODE, DIVISION_CODE, CITY_CODE, dist_cn_name,dist_en_name FROM tm_new_district t WHERE t.division_code IS NOT NULL 新的sql查询耗时只有不到1s，因此，大量查询导致数据库压力飙升的问题得到了解决 上线后优化效果 优化前CPU压力图（单节点）： 手段1优化后： 结合手段1、手段2，优化后： 结论 经过优化后，数据库压力得到了很大的缓解，突发峰值也基本消失；应用CPU压力也从原来的15%降低到3%左右，整体系统性能得到了很大的提升。 结语 这次分享了：缓存短时间密集加载与sql慢查询（姑且算是吧）的处理。算是很经典的问题了，到处都会遇到，希望能帮助到大家。","link":"/Bug-Log-Optimization/cache-load-database-crash.html"},{"title":"每天进步一点点（持续更新）","text":"[原创]这篇只是做点记录备忘，个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Make-A-Little-Progress-Every-Day/Make-A-Little-Progress-Every-Day.html 2022-10-25 目前本地缓存使用的方式 订运单系统： SF自研的类Ehcache框架，存储的内容不是特别多，都是一些网点月结卡号信息，所有对象存在于Map，占200m左右 SISP系统： 使用了Caffeine，存储的内容，存储的内容很多，包括员工表（900m），人员表（bdus 800m） 客户表（1.2g），用户及权限相关表（500m） 关联后约占4g内存，目前采用Caffeine默认存储方式，启动即全量加载（极个别采用懒加载方式加载），全部存在于Map中，即堆存储 优化: 缓存为基础数据，数据量稳定，目前采用CMS回收器，堆空间8g，缓存存于堆中，占约4g，平时MajorGC达4~6s，曾出现高峰gc达52s， 应考虑将缓存存于堆外，减少GC的压力，提升性能（风险点，可能会导致内存溢出） 后面自己论证时行不通...因为java对象存到堆外时需要额外进行序列化，经测试，这会导致对象明显变大，浪费的内存有点多，在降本增效的背景下是行不通的 参考 guava、caffeine、ohc（堆外缓存）详解 2022-10-20 Spring中最常用的11个扩展点 Spring中最常用的11个扩展点 2022-09-02 Hystrix熔断配置 为让Hystrix的熔断降级配置更加合理，会议讨论结果需进行如下优化， 为每个已有Hystrix熔断的接口设置最高并发配置（execution.isolation.semaphore.maxConcurrentRequests），配置200~500之间，具体计算方式 单节点线程数 = QPS /节点数/ ( 1000 / 被熔断方法的P99耗时ms ) 把Hystrix配置提取到disconf，重启生效，无需发版 QPS和RT的关系： 对于单线程：QPS=1000/RT 对于多线程：QPS=1000线程数量/RT 对于多线程多接点：QPS=1000单节点线程数量*节点数量/RT 2022-08-11 前端跨域请求减少Option请求 后端对CorsConfiguration配置Access-Control-Max-Age，前端请求时接收到Access-Control-Max-Age，在该有效时间内不会再发出Option请求 CorsConfiguration config = new CorsConfiguration(); config.setMaxAge(600L); 后端返回的Access-Control-Max-Age 大于浏览器支持的最大值 那么取浏览器最大值作为缓存时间 否则取后端返回的Access-Control-Max-Age作为缓存时间 缓存时间内不会再发option请求 源码 2022-06-03 POJO、JavaBeans、BO、DTO 和 VO 、DO之间的区别 POJO，也称为普通旧 Java 对象，是一个普通的 Java 对象，它没有对任何特定框架的引用。 JavaBean/BO：有约束的POJO，国内用法一般为BO 实现Serializable接口 将属性标记为private 使用 getter/setter 方法来访问属性 DTO：也称为数据传输对象，封装值以在进程或网络之间传输数据。 DTO 没有任何显式行为。它基本上有助于通过将域模型与表示层解耦来使代码松散耦合 VO：外国作为值对象，不过国内用法是用来做视图对象，主要是返回前端用的对象 DO(Data Object) ，持久化对象，数据库对象 2021-06-01 System.arraycopy方法和Arrays.copyOf() System.arraycopy方法：是本地方法，如果是数组比较大，那么使用System.arraycopy会比较有优势，因为其使用的是内存复制，省去了大量的数组寻址访问等时间 Arrays.copyOf() Arrays.copyOf()在System.arraycopy()实现的基础上提供了额外的功能 会创建新数组 允许与原数组类型不同，但是这样会调用JVM的反射，性能较差 2021-05-20 ES 分词 text：用于全文索引，该类型的字段将通过分词器进行分词，最终用于构建索引 keyword：不分词，只能搜索该字段的完整的值，只~~~~用于条件精准查询 通常情况都以 keyworkd 字段进行搜索，因为全文索引的分词器不一定能够完全分词，可能会导致搜索不准确，所以一般都是用 keyword 字段进行搜索 2021-02-26 HBASE 列族,RowKey HBase是一种面向列的数据库,以row+列名作为key，data作为value，依次存放 假如某一行的某一个列没有数据，则直接跳过该列。对于稀疏矩阵的大表，HBase能节省空间 表是行的集合 行是列族的集合 列族是列的集合 列是键值对的集合 2021-02-08 最近要搞懂的事情 MySQL、HBase、ES的特点和区别 redo log和checkpoint机制 单机情况下，MySQL的innodb通过redo log和checkpoint机制来保证数据的完整性。因为怕log越写越大，占用过多磁盘，而且当log特别大的时候，恢复起来也比较耗时。而checkpoint的出现就是为了解决这些问题。 mysql主从架构 Master-Slave(主挂了可能会丢失一部分数据)和Group Replication 的架构(mgr采用paxos协议实现了数据节点的强同步，保证了所有节点都可以写数据，并且所有节点读到的也是最新的数据) 2021-02-07 稍稍记录一下2020年干过的那些P大点的事 协助完成Redis降存储–&gt;阉割无用字段,(没用上压缩) ,以前是存储整个对象,现在是存储个别有用的字段, 降低了60%~80%的存储 综合订单、CX、操作运单、公共redis，共节省redis资源9034G 团队共同完成灰度发版–&gt;中间加应用,数据先到分流应用,通过分流应用把对应城市、网点的数据分流到对应的应用 独立完成ES查询优化–&gt;优化判断索引逻辑,指定查询具体某个分片,提高性能550倍 生产某个节点线程数过多及CPU高–&gt;dump&amp;排查源码 elasticJob的采用了流式处理,有某个节点的一些线程一直能查到数据,就一直继续工作了; elastic-job流式处理导致最终只有一个线程在跑的问题排查&amp;修复 —&gt; 同上 重试模块加入根据重试次数逃生逻辑,防止异常时空跑把系统跑死 优化ES存储订单数据的结构 —&gt; 4亿+数据量减少到只剩下5kw数据量，降低了十倍左右 把orderExtendInfoList类型改为keyword类型（原来为）, 内部额外存储一个作为索引用的值为原orderExtendInfo的key和value对应的Map 描述起来比较麻烦 大概是把下图左边的变成变成右边的 数据造就业务—&gt;咋玩??? 目前手上有啥数据: 订单–&gt;可以对BSP客户进行分类, 对不同类型客户,可以特别推荐一些增值服务或产品 -----&gt;根据寄件商品的类型为其推荐增值服务 扩展信息…没啥用 增值服务 订单状态&lt;—监控? 存在很多很久不揽收的 进行告警通知小哥? 让其决定是取消，还是让其再设定一个较远的预约时间 FVP所有状态&lt;– 运单号生成 运单&lt;— 产品变更&lt;— 变更监控? 至少可以记录一下产品变化以及运费变化 2021-01-25 一、ZK事件回调原理 – 最近用得少老是忘记，还是记录一下吧 简单来说，就是客户端启动后，会在zk注册一个watcher监听某个我们关心的节点Node的变化； 同时客户端会把这个watcher存到本地的WatcherManager里; 当这个节点出现变化，zk会通知到对应的客户端，调用该watcher的回调方法（process方法）。 以此方式实现动态配置平台的配置刷新下发、分布式锁等功能 2020-12-14 一、 ElasticSearch原理 图解ElasticSearch原理 精确查询 term 查询是如何工作的？ Elasticsearch 会在倒排索引中查找包括某 term 的所有文档 Lucene Index(包含多个Segments)： Segments 是不可变的（immutable）： Segments Delete？当删除发生时，Lucene 做的只是将其标志位置为删除，但是文件还是会在它原来的地方，不会发生改变。 Segments Update？所以对于更新来说，本质上它做的工作是：先删除，然后重新索引（Re-index） 随处可见的压缩：Lucene 非常擅长压缩数据，基本上所有教科书上的压缩方式，都能在 Lucene 中找到 缓存所有的所有：Lucene 也会将所有的信息做缓存，这大大提高了它的查询效率 整体结构 Cluster由多个Node节点组成 每个Node节点由多个索引Index组成 每个索引由多个Share组成 每个Share(又叫Lucene Index)存在于集群中多个Node中,具体有多少个Share,看你索引的配置,由多个Segment组成 每个Segment(又称Mini索引),每个Segment都是不可变的,只会生成一个增量Segment(含修改后的/新增的数据),原来的数据只能标记为删除,当Segment多了之后会做merge合并操作; Segments的创建&amp;刷新 (没玩大数据 大概了解就行了) 进行索引文档后,看是否有达到flush条件的Segment,存在就flush该Segment将该数据刷到硬盘中,没找到就创建一个Segment?? 参考 -&gt; ES lucene写入流程，segment产生机制源码分析 2020-11-18 一、 MYSQL是怎么运行的 – 连接原理 –以下为内连接,驱动表为t1,如果t1通过where过滤完还有2条数据,那么会去t2表查询2次 select * from t1 join t2 where ***; select * from t1 inner join t2 where ***; select * from t1 cross join t2 where ***; (以上等价于)select * from t1,t2 where ***; select * from t1 left join t2 on t1.a=t2.a where ***; – 为外连接 on实际是给外连接用的,在内连接使用的话和where的作用是一样的; 在外连接中使用,如果匹配不上,不会过滤掉驱动表原有的值;如果要过滤掉这种连接不上的值,可以再加个where条件过滤 驱动表t1只会被访问一次，被驱动表t2会被访问多次 2020-09-24(好久没做记录了…) 一、 DB 看似匹配到索引,但是没有走索引的情况(注意事项) 因类型转换导致不走索引 摘自本文结论内容 建表语句cell的数据类型为Varchar create table t ( id int(20) primary key AUTO_INCREMENT, cell varchar(20) unique )engine=innodb; 建表的时候cell定义的是字符串类型 Explain 通过explain，基本已经可以判断： update t set cell=456 where cell=55555555555; 并没有和我们预想一样，走cell索引进行查询，而是走了PK索引进行了全表扫描。 实际问题 where语句cell类型与索引的不匹配，不会走索引，最终会走全表； 结论 类型转换，会导致全表扫描，出现锁升级，锁住全部记录 二、 DB 执行计划查看&amp;&amp;死锁排查 执行计划 select_type：SIMPLE 这是一个简单类型的SQL语句，不含子查询或者UNION。 type：index 访问类型，即找到所需数据使用的遍历方式，潜在的方式有： （1）ALL（Full Table Scan）：全表扫描； （2）index：走索引的全表扫描； （3）range：命中where子句的范围索引扫描； （4）ref/eq_ref：非唯一索引/唯一索引单值扫描； （5）const/system：常量扫描； （6）NULL：不用访问表； 上述扫描方式，ALL最慢，逐步变快，NULL最快。 possible_keys：NULL 可能在哪个索引找到记录。 key：PRIMARY 实际使用索引。 ref：NULL 哪些列，或者常量用于查找索引上的值。 rows：5 找到所需记录，预估需要读取的行数。 死锁排查 有权限的mysql账户执行: show engine innodb status; 根据查到的结果 分析LATEST DETECTED DEADLOCK里的内容 三、ES 提高查询效率 学习自: ES 在数据量很大的情况下（数十亿级别）如何提高查询效率 把尽可能多的索引放在filesystem cache中 不做复杂查询（Join等），如果有这样的需要，应以设计得更好的document（记录）来实现简单查询（单表） 使用ES+hbase架构: ES存索引，索引全放在filesystem cache，数据存HBase；通过ES进行条件查询，获取docId，用该docId去查HBase 禁止深度查询，使用scrollApi或search_after代替 四、ES存储结构 index -> type -> mapping -> document -> field 实例： order~2020-08-02/order/_mapping/记录/字段 翻译： 索引名称/表名/表结构/记录/字段 2019-12-19 Kafka 基础点 Topic&amp;消费组: 一个Topic的一个Partition只能一个Consumer Group的一个节点消费 一个【Topic】对应多个【Partition】(文件) 消息大小限制: 一条消息 默认最大只能为1000000B(976.56 kB),所以一般规定不允许发送&gt;900k的消息 版本区别: 0.8版本 (相对历史版本 支持了Replication高可用 ) 当时只有Consumer Coordinator coordinator需要依赖于ZK，通过zk监听/consumers//ids变化 与 brokers/topic的数据变化决定是否要 rebalanced rebalanced后,consumer自己决定自己要消费哪些Partition，然后抢先在/consumers//owners//下注册（通过这种方式实现一个Topic的一个Partition只能一个Consumer Group的一个节点消费`） 同时,各个Consumer Coordinator还需要进行位移的提交 弊端: 消费者自己决定消费哪些分区,各个Consumer Coordinator还需要进行位移的提交 并且分区的决定与位移的提交都需要依赖于ZK 0.8.2版本 0.8.2版本开始同时支持将 offset 存于 Zookeeper 中与将offset 存于专用的Kafka Topic 中,但是需要High Level API的支持，且BUG较多，目前公司用的还是Low Level Api 0.9.x版本 新增Group Coordinator,存在于Broker端 代替了0.8.x版本的zk，每个消费组对应一个，负责每个消费者位移的提交&amp;分区消费的决策 0.10+ 消息结构添加了时间戳，可根据这个时间戳实现延迟队列 0.11.x版本 新增了对【幂等】、【事务】的支持(依赖于Producer幂等) (exactly-once) 3.High Level和Low Level 将仅支持zookeeper维护offset方式的 高级抽象的API称为 Low Level Api,高度抽象, 将支持kafka broker 维护offset方式 抽象低的API的称为 High Level API ， High level consumer vs. Low level consumer 官方解释(看最下面的描述) 消息(生产)幂等 每个Topic的每个Partition对每个生产者都维护了一套ID(UUID) 生产者每次发送消息时候,消息体都带上这个ID+1，以此Broker可得知： 当消息的squence number等于broker维护的squence number + 1，表示消息有序且第一次消费 当消息的squence number小于或等于broker维护的squence number，表示重复消费额 当消息的squence number等于broker维护的squence number + n（n &gt; 1），表示存在消息丢失 参考1:Kafka Producer 幂等的原理 参考2:上半场的幂等性设计 消息的分区选择: 一条消息会根据Key被路由到某一【Partition】（key=0对应分区0）；如果没有指定key，消息会被均匀的分配到所有分区；目前我们封装的方案是，不管有没有Key，都会被随机打乱到每个分区） 每隔 topic.metadata.refresh.interval.ms 的时间，随机选择一个partition。这个时间窗口内的所有记录发送到这个partition。发送数据出错后也会重新选择一个partition 对key求hash，然后对partition数量求模: Utils.abs(key.hashCode) % numPartitions 代码: kafka.producer.async.DefaultEventHandler#handle Kafka支持的消息发送模式 At most once 消息可能会丢，但绝不会重复传输(例:读到先Commit,再处理) At least one 消息绝不会丢，但可能会重复传输(例:读到先处理,再Commit) Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的 (0.8.2版本还不支持) 高可用 kafka默认会重试3次 零碎小点 Kafka实现的是客户端软负载: 让producer决定丢到哪个partition里 Consumer端仅支持pull模式，这也有利于让Consumer端决定消费速率 Consumer不能消费太久(如Sleep),因为Kafka会认为程序宕了,分区会重新进行分配,把消息分给其他的Consumer (相关配置项: max.poll.interval.ms) Consumer每次可从Kafka取max.poll.records条数据进行处理 如果想要消息有序 那么就得保证同个业务key的消息都是发到1个分区里 Redis-Sentinel&amp;Jedis 通过Sentinel集群获取Redis主节点原理 SF-Sentinel中配置Redis链(mymaster1,mymaster2,mymaster3)，然后获取每一条链的Master，进行初始化Redis连接池 原生的Sentinel中配置Redis链，然后获取该链的Master，进行初始化Redis连接池 Jedie的Key是如何被存入Redis的某个节点的 参考:Jedis之ShardedJedis一致性哈希分析 Jedis初始化时会初始化160个虚拟节点，160个虚拟节点通过Map（Map&lt;ShardInfo, R&gt; resources）映射到实际的Redis-Master节点 Jedis在Set key时会对Key分片计算（计算落在160个节点的哪一个），然后再根据虚拟节点与实际节点的映射，把指令发给实际的节点 参考代码： redis.clients.util.Sharded#initialize redis.clients.util.Sharded#getShard(byte[]) Redis-Sentinel模式是如何扩容的 空 Jedis一致性分析 2019-11-28 git rebase -i HEAD~2 pick：保留该commit（缩写:p） reword：保留该commit，但我需要修改该commit的注释（缩写:r） edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e） squash：将该commit和前一个commit合并（缩写:s） fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f） exec：执行shell命令（缩写:x） drop：我要丢弃该commit（缩写:d） Hibernate 基本知识 inverse属性表示本实体是否拥有主动权 inverse只有在非many方才有，也就是many-to-many或者one-to-many的set,List等 2019-11-25 数据库count() 官方解释 Returns a count of the number of non-NULL values of expr in the rows retrieved by a SELECT statement. The result is a BIGINT value. 返回行中 expr 的非 NULL 值的计数 count(*) 和 count(1) 5.7.18以后，两个函数执行计划都是一样的 如果该表没有任何索引，那么会扫描全表，统计行数 如果该表只有一个主键索引，没有任何二级索引的情况下，那么通过主键索引来统计行数的 如果该表有二级索引，则会通过占用空间最小的字段的二级索引进行统计 count(column） 如果字段定义为not null，则按行累加，如果允许有null，则会把值取出来判断一下是不是null，将不是null的值累加返回。 MyISAM 与 InnoDB MyISAM会记录每个表的行数，count()时直接返回 InnoDB会通过扫描全表或索引，得到行数 在使用count函数中加上where条件时，在两个存储引擎中的效果是一样的，都会扫描全表计算某字段有值项的次数 DB select count速度 count(*)=count(1)&gt;count(primary key)&gt;count(column) 参考 MySQL原理：count(*)为什么这么慢，带你重新认识count的方方面面 2019-11-22 [垂直]分库分表 目标 通过减少数据量，提升性能 原则 长度较短，访问频率较高的属性尽量放在一个表里，我们将其称为主表(base表) 字段较长，访问频率较低的属性尽量放在一个表里，我们将其称为扩展表(ext表) 经常一起访问的属性，也可以放在一个表里(备选) 大数据量场景注意事项 不能用Join 解决方式: 让应用自己拆分成两次查询 base表和ext表不能Join，因为一旦Join了，那么两张表就出现了耦合，这不利于日后拆表到别的数据库实例上 Join很消耗数据库的性能(分布式场景下,瓶颈往往是数据库) 提高性能的原理 减少单表的数据量，减少磁盘IO（降低每行记录大小） 更好的利用缓存 因为减少单表数据量还可以充分利用数据库缓存，减少磁盘IO 2019-11-20 数据库基本知识 MyISAM与InnoDB索引的区别 MyISAM: MyISAM不存在聚集索引,主键索引与普通索引没区别，叶子节点都是存储的都是数据的地址 InnoDB: InnoDB必然有[一个]聚集索引（为主键索引,没主键时会用第一个非空普通索引，都没有会生成一个基于行号的聚集索引） select * from t where name=‘lisi’; 会先通过name辅助索引定位到B+树的叶子节点得到id=5，再通过聚集索引定位到行记录 违反唯一索引场景: MyISAM会出现一个update语句，部分执行成功，部分执行失败(因为不支持事务) 2019-11-11 Elastic-Job 运行规则: 3台机器的一个集群 ,shardingCount=10 ,分片结果为：1=[0,1,2,9], 2=[3,4,5], 3=[6,7,8] (参考AverageAllocationJobShardingStrategy) 如果本机的数据分片分到了多个分片（即一个JVM进程分到了多个分片），则Elastic-Job会为每一个分片去启动一个线程来执行分片任务 线程: 每个任务对应一个线程池,其默认线程数为: 2*逻辑核心数(参考DefaultExecutorServiceHandler) 线程池配置为: new ThreadPoolExecutor(threadSize, threadSize, 5L, TimeUnit.MINUTES, workQueue, new BasicThreadFactory.Builder().namingPattern(Joiner.on(&quot;-&quot;).join(namingPattern, &quot;%s&quot;)).build()); (参考ExecutorServiceObject) 问题: 要注意单机线程数要 大于 单机获取到的分片数 - 参考 《Elastic job 线程模型 源码分析》 一个jvm实例 处理多个 job ， 每个job 在该实例上分片数又大于逻辑核心数*2 的数量 随着job不断增加 ， 单个job任务执行时间可能会变长 ，有可能超过平时的任务完成超时时间 ，造成任务失败 举个例子: 如果一台机器 处理器数 2 ， 线程池 就是 4 ， 如果 分片是 5 ， 就是说 一个分片会被排队 ，实际完成时间 &gt;2 个分片 完成时间 Elastic-Job其他 1. 失效转移 - 参考 【简单的HA】版失效转移 (默认) 在作业节点下线，或者zk的session超时（默认60s）时，会在下一轮任务分片时，把这个该问题节点的分片分给别的正常节点进行作业 （可能会存在作业重复处理的问题） 【真正的】'失败’转移 (需要开启) 当failover（默认值为false） 配置为true时，才会启动真正的失效转移； 当failover（默认值为false） 和 monitorExecution（默认值是true）这两个配置都为true时 只有对monitorExecution为true的情况下才可以开启失效转移； 如果任务1在A节点执行【失败】，那么会【转移】给别的存活的节点【竞争】执行这个任务1； - 参考 - 官方参考 2019-08-26 官方参考 MySQL锁 InnoDB锁机制是基于索引建立的 如果SQL语句中匹配不到索引,那么就会升级为表锁 记录锁 1234-- id 列为主键列或唯一索引列SELECT * FROM table WHERE id = 1 FOR UPDATE;或update table set age=2 WHERE id = 1; 通过唯一索引实现的记录锁,只会锁住当前记录(必须为=不然会退化为临键锁) 间隙锁 间隙锁只有在事务隔离级别 RR(可重复读)中才会生效. 为非唯一索引组成(如class,age等) 1select student where age&gt;26 and age&lt;28 lock in share mode ; -- 这里以读锁为例 使用间隙锁的条件 命中普通索引锁定； 使用多列唯一索引； 使用唯一索引命中多行记录 临键锁(Next-key Locks) 临键锁只有在事务隔离级别 RR(可重复读)中才会生效. 是记录锁与间隙锁的组合 可以是唯一索引,也可以是非唯一索引,对其都以间隙锁的形式进行锁定(以唯一索引匹配,并且只匹配到一条数据除外) 临键锁(Next-key Locks) 例子: tno(唯一索引) tname tsex tbirthday prof depart age(非唯一索引) 858 张旭 1 1969-03-12 讲师 电子工程系 25 857 张旭 女1 1969-03-12 讲师 电子工程系 25 856 张旭 男 1969-03-12 讲师 电子工程系 25 831 刘冰 女 1977-08-14 助教 电子工程系 29 825 王萍 女 1972-05-05 助教 计算机系 28 804 李诚 男 1958-12-02 副教授 计算机系 26 其中有唯一索引的临键为: (-∞,804] (804,825] (825,831] (831,856] (856,857] (857,858] (858,+∞] 其中有非唯一索引的临键为: (-∞,25] (25,26] (26,28] (28,29] (29,+∞] 非唯一索引临键锁验证 123-- session1select * from teacher WHERE age between 26 and 28 lock in share mode ; 这时候会锁定非唯一索引的临键 (25,29] 所以我们测试更新age=25–&gt;成功 插入age=27阻塞 更新age=29阻塞 插入age=30成功即可验证 12345678910111213-- session2-- 更新age=25--&gt;成功update teacher set tsex='女1' WHERE age=25;-- 插入age=27阻塞insert into `test`.`teacher` ( `tno`, `tname`, `tsex`, `tbirthday`, `prof`, `depart`,`age`) values ( '740', '张旭1', '12', '1969-03-12 00:00:00', '讲师', '电子工程系',27);-- 更新age=29--&gt;阻塞update teacher set tsex='女1' WHERE age=29;-- 更新age=30--&gt;成功insert into `test`.`teacher` ( `tno`, `tname`, `tsex`, `tbirthday`, `prof`, `depart`,`age`) values ( '740', '张旭1', '12', '1969-03-12 00:00:00', '讲师', '电子工程系',30); 唯一索引临键锁验证 12-- session1select * from teacher WHERE tno between &quot;831&quot; and &quot;856&quot; lock in share mode ; 根据上面的sql,我们匹配到唯一索引临键锁为:(825,857] 所以我们测试更新tno=825–&gt;成功 更新tno=857阻塞 更新age=858成功即可验证 123456-- 更新tno=&quot;825&quot;--&gt;成功update teacher set tsex='女1' WHERE tno=&quot;825&quot;;-- 更新tno=&quot;857&quot;--&gt;阻塞update teacher set tsex='女1' WHERE tno=&quot;857&quot;;-- 更新tno=&quot;858&quot;--&gt;成功update teacher set tsex='女1' WHERE tno=&quot;858&quot;; 2019-08-22 Spring事务/AOP增强 @EnableAspectJAutoProxy(exposeProxy = true) 进入代理时,通过AopContext.serCurrentProxy(proxy)把当前代理设置到ThreadLocal中 后续在线程销毁(请求结束)前调用代理内部之间的调用就可以通过((AService)AopContext.currentProxy()).b()进行调用了 PS. 性能影响不大 不过实际上代理内部之间还需要AOP增强的场景不多,一般没必要用 Spring LTW实现的静态织入（应该不能叫做代理） 需要添加配置： 代码添加: @EnableLoadTimeWeaving(aspectjWeaving=ENABLED)或&lt;context:load-time-weaver aspectj-weaving=&quot;enable&quot; /&gt; 添加JVM参数-javaagent:类加载器代理路径 LTW(LoadTime Weaving) 加载时织入。在通过JVM加载类时候会先调用ClassTransformer的transform()进行字节码替换后才会进行加载。 静态AOP 通过LTW可以实现静态AOP增强，加载到的类就是已经增强后的代码。这样我们调用方法的时候,直接就是调用了增强后的方法,比起动态代理的调用,更加地高效。 上述流程大致如下所示: graph TD A[Target] B[增强后的字节码] C[加载后的代码] D[注入后的Bean] E[调用方] A--ClassTransformer的transform方法进行字节码植入-->B B--JVM加载-->C C--Spring使用,创建/注入Bean-->D E--方法调用-->D 2019-08-01 Spring事务 对于this.b()这些类实例的内部调用，b()实际上是无事务的 但是可以用((AService)AopContext.currentProxy()).b() 结合@EnableAspectJAutoProxy(exposeProxy = true) 这样b()就包裹在事务里了 2019-7-20 seata seata需要管理所有的数据库操作，不然不能通过前镜像进行回滚 2019-7-17 Spring事务/Cglib final,static,private修饰符无法被增强 由于使用final,static,private修饰符的方法都不能被子类覆盖，相应的，这些方法将不能被实施的AOP增强 增强应该作用在实现类中 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。 2019-5-20 【GC日志】GC耗时解析 【Time: user=0.71 sys=0.01 real=0.02 secs】 user表示：本次GC过程中【所有线程】在用户态消耗的时间总和 sys表示： 本次GC过程中 【所有线程】在内核态所消耗的时间总和 real表示：本次GC过程中，实际GC消耗的时间 2019-5-1 数据库MVCC MVCC：多版本并发控制(Multi-Version Concurrency Control) 优势：查询速度快，并发环境尤是。对于大多数读操作，我们只需要通过MVCC进行简单的查询操作，而不需要获取任何一个锁。 劣势：需要多存储数据。对每一条记录都需要存储所有版本的数据 MVCC只工作在REPEATABLE READ和READ COMMITED隔离级别下 READ UNCOMMITED不是MVCC兼容：因为这个模式只能读取到最新的数据 SERIABLABLE也不与MVCC兼容：因为每个读操作都需要为读到的数据上锁 MVVC机制： 以下摘自《五分钟搞清楚 MVCC 机制》 每一条数据库表记录,都隐藏2个字段 数据行的版本号 （DB_TRX_ID） 删除版本号 (DB_ROLL_PT) 执行insert语句插入的时候,会把当前的事务ID写到该记录的数据行的版本号 （DB_TRX_ID）中: 123begin;-- 获取到全局事务ID 假设为2insert into `test_zq` (`id`, `test_id`) values('5','68');commit;-- 提交事务 id test_id DB_TRX_ID DB_ROLL_PT 5 68 2 NULL 6 78 1 3 修改数据库记录的时候 更新原记录的删除版本号 (DB_ROLL_PT)为当前事务ID 插入一行新的更新后的记录,且它的数据行的版本号 （DB_TRX_ID）为当前事务ID 123begin;-- 获取全局系统事务ID 假设为 10update test_zq set test_id = 22 where id = 5;commit; id test_id DB_TRX_ID DB_ROLL_PT 5 68 2 10 6 78 1 3 5 22 10 NULL 查询的时候需要根据数据行的版本号 （DB_TRX_ID） 和 删除版本号 (DB_ROLL_PT) 二者进行数据数据筛选，需要同时满足以下规则： 数据行的版本号 （DB_TRX_ID） &lt;= 当前事务 删除版本号 (DB_ROLL_PT) &gt; 当前事务 123begin;-- 假设拿到的系统事务ID为 10select * from test_zq;commit; id test_id DB_TRX_ID DB_ROLL_PT 6 22 10 NULL 2019-04-24 Spring的Lifecycle (SpringAppilication生命周期) Spring会拿到所有Lifecycle实现类，然后委托DefaultLifecycleProcessor进行逐个处理 Lifecycle 可以在SpringAppilication在初始化后执行start()方法,Spring停止的时候调用stop()方法 但是单单实现该类不能实现SpringAppilication在启动后,停止时调用Lifecycle对应的方法 这时候我们应该需要使用SmartLifecycle（Lifecycle的子类）,重写isAutoStartup()返回true，才能产生理想效果 2019-04-23 关于测试类的规范 单元测试应该是不依赖于别的单元测试的 所有单元测试应该都得回滚，如果存在异步处理的情况，应尽可能把主线程与fork线程拆成2个测试类方法进行测试 每个测试类／测试方法应写上对应的名称@DisplayName 每个接口，都必须写一个正向测试方法 关于测试类的类名：测试类与被测试的类的路径需要一致，名字也需要对应，如： 123com.fpx.wms.service.impl.InstockServiceImpl↓对应↓com.fpx.wms.service.impl.InstockServiceImplTest 关于测试类的方法名： 方法名尽可能为成功的条件如shouldSuccessAfterPay()，而方法具体用来测试哪个场景的，我们已经使用了@ DisplayName来描述，无须担心 对于结果，需要适应assert断言输出与结 2019-04-22 Spring @Lookup 作用 在单例A里 可能依赖到原型类型B,这时候如果用普通的Autowrite不能拿到原型的B，这时候就需要使用@Lockup了 使用参考 参考地址 官网地址参考地址 2019-04-21 架构设计三大原则 合适原则 简单原则 演化原则 即，合适优于先进，简单优于复杂，演化优于一步到位 →能不分，尽可能不分 2019-03-20 策略模式 vs 命令模式 1. 策略模式 1策略模式针对一个命令,多种实现方式 2. 命令模式 1命令模式针对多个命令,每种命令都有各自的实现 3. 总结 1命令模式等于菜单中的复制，移动，压缩等，而策略模式是其中一个菜单的例如复制到不同算法实现。 2019-03-15 策略模式 vs 代理模式 1. 策略模式 1需要调用方告知具体的策略 2. 代理模式 12需要调用方告知使用哪个[代理类]具体的【被代理类】由【代理类】生成，客户端不知道具体被代理的是谁 2.1 动态代理 1需要调用方告知[被代理类]及其接口 3.One More Thing 12以上模式都需要客户端告知具体的[策略]/[代理]/[被代理者]为了使实现其与调用方进行隔离,可以使用[**工厂模式**]进行隔离 2019-03-12 Spring循环依赖 场景现有3个类相互依赖，依赖关系分别为： graph LR A-->B B-->C C-->A 场景细分为3种 构造注入参数循环依赖(报错) 报错 根据Spring初始化方式,Spring容器会按照顺序创建&quot;无属性&quot;的A放到“当前创建Bean池”中，同理再B、C、A，但是在再次创建A的时候发现“当前创建Bean池”已经存在A了，那么这时候会报错循环依赖 Setter注入的循环依赖(单例) 没毛病，在set的时候对象ABC都已经实例化放在Spring缓存了好了 Setter注入的循环依赖(prototype) 报错 prototype修饰的bean不会被Spring缓存,都是使用的时候当场创建的 Spring注入方式选择 结合上面的循环依赖问题，Setter出现问题的概率会低一些 推荐使用Setter注入 构造注入 Setter注入 接口注入(没用过) 2019-03-11 一、集合操作 遍历 Enumeration(JDK1.0) 只提供读集合相关功能，因为没有fail-fast，速度较快一点 Iterator(推荐) 除了读功能，还有删除集合元素的能力，并且支持fail-fast（防止多线程同时对集合修改的一种机制） 修改 正例： 以List为例子,先得获取他的Iterator,通过iterator来进行修改操作 反例： 使用增强型foreach进行add/remove操作： 因为增强型foreach实际上是使用iterator实现的java语法糖: 1234567891011List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() {{ add(&quot;test1&quot;); add(&quot;test12&quot;); add(&quot;test13&quot;); add(&quot;test14&quot;);}};for (String userName : userNames) { if (userName.equals(&quot;test12&quot;)) { userNames.remove(userName); }} 编译后 12345678910111213141516List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() { { this.add(&quot;test1&quot;); this.add(&quot;test12&quot;); this.add(&quot;test13&quot;); this.add(&quot;test14&quot;); }};Iterator var1 = userNames.iterator();while(var1.hasNext()) { String userName = (String)var1.next(); if (userName.equals(&quot;test12&quot;)) { userNames.remove(userName); }} 123456所以实际上for (String userName : userNames) 这里每次都会去调用itertor.next()如果你在迭代期间,操作了list.add()和list.remove()等不通过Iterator的操作next()里会去调用checkForComodification()方法然后发现modCount != expectedModCount 抛出异常因为list.add()和list.remove()等不通过Iterator的操作,是不会修改expectedModCount的 其它 fail-fast： 防止多线程同时对集合修改的一种机制 modCount： ****List**中的一个成员变量。它表示该集合实际被修改的次数 expectedModCount： 是 ****List**中的一个内部类——Itr中的成员变量 二、Hystrix Feign-starter包含Hystrix以及ribbon(只用他的均衡负载 http请求还是用feign自己的) 一个@FeignClient对应一个线程池或信号量 隔离 线程池隔离 tomcat的请求线程会交给线程池的线程处理 超过线程池会排队或者降级，一个线程池对应的服务挂了，不会影响别的线程池的服务 信号量隔离 只作为开关 并发数超过X服务的信号量,多出来的Tomcat请求将会被拒绝 2019-03-05 一、StringBuilder在高性能场景下的正确用法 StringBuilder在高性能场景下的正确用法(文中代码打错了一些字…) 正确写法应该是这样↓ StringBuilderUtil.java 2019-03-01 一、分布式锁 从需求上说，分布式锁要求是不一样的： 如果是用于聊天等社交场景,那么可以使用AP的分布式锁:Redis 如果是用于交易等不允许极端情况下获取锁不一致的，那么AP的Redis锁是不能接受的，这时候一定得用CP的分布式锁,如:etcd Zookeeper这一类 2019-02-22 一、ThreadLocal 每个线程都有一个ThreadLocalMap,ThreadLocalMap以Entry的形式保存着各个线程自己的数据 Entry为一个WeakReference,以你new的ThreadLocal为Key 基于2.当你new的ThreadLocal没被外部强引用时,线程该Thread下对应该ThreadLocal的Entry会在下次GC被回收 当一条线程创建了多个ThreadLocal，多个ThreadLocal放入ThreadLocalMap 会极大地增加冲突概率 ThreadLocalMap对冲突的处理方式与普通HashMap的链表处理不一样，而是以原来的位置+1，一直寻找到没有冲突的地方存入 ThreadLocal在ThreadLocalMap中是以一个弱引用身份被Entry中的Key引用的 ThreadLocal.remove(),移除ThreadLocalMap与Entry的关系，释放内存 2019-02-17 一、常量池 常量池包含: class常量池 存在于class文件中 运行时常量池 存在于方法区中 一个类对应一个运行时常量池 字符串常量池 全局唯一 JDK6存在于方法区(独立于运行时常量池) JDK6以后存在于堆中 二、字符串加载到字符串常量池的2种方式 graph LR A[编译后的class文件中的class常量池] B[运行时常量池*N] C[字符串常量池] D[Java代码运行] A-->B D-->B B-->C 2019-01-28 Mybatis 一级缓存 (范围为一个SqlSession) 有Session/STATEMENT级别: 默认是SESSION 级别，即在一个MyBatis会 话中执行的所有语句，都会共享这一个缓存。 一种是STATEMENT 级别，可以理解为缓存只对当前执行的 这一个Statement 有效 二级缓存 基于mapper 二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享 补充: 缓存为本地缓存, 在集群部署的系统里开启后,会导致A1查询与A2查询结果不一致的问题 看情况开启,一般为关闭;或者使用Redis等工具使用统一的第三方缓存 2018-11-06 一、 分布式事物要看场景的 举个例子: 流量充值涉及到订单支付，金钱交易严格用tcc; 订单支付完后要给用户增加积分，这个必要成功，用最终消息一致性方案; 订单支付完后还要给用户发送一条短信，短信一般是跟电信运营商的第三方接口对接，有可能成功有可能失败，用最大努力通知方案 2018-11-06 一、JVM逃逸分析与TLAB(Thread Local Allocation Buffer) 启动逃逸分析后,会分析没有逃逸的对象,把没有逃逸的对象分配在线程私有的栈里,性能提高5倍 TLAB(默认开启)存在于新生代,默认占其1%,为线程私有;因为线程私有,没有锁开销(对象分配的时候不需要锁住整个堆),效率高; 创建对象时内存分配流程: 逃逸分析,确定分配在哪,如果是分配在堆则2 尽量分配在当前线程的TLAB,不够就去再申请一个TLAB,还不够则3 加锁Eden区,在Eden申请内存,不够则4, 执行Young GC Young GC后,如果还不够,放入老年代 对象分配流程写的不错 参考:https://blog.csdn.net/yangzl2008/article/details/43202969 2018-10-19 一、Feign Consul 获取可用服务IP HealthConsulClient.getHealthServices获取可用IP 通过http://consul.uat.i4px.com:8500/v1/health/service/pds-pos-outer?token= 最终会在ConsulServerUtils.findHost()得到服务所对应的可用IP IP获取逻辑是: 获取Service.Address字段作为可用IP 取不到就取Node.Address 二、Consul 1.服务注销/删除 http://consul.uat.i4px.com:8500/v1/agent/service/deregister/fpx-prs-service-10-104-5-15-8002 2.查看可用服务 http://consul.uat.i4px.com:8500/v1/health/service/wims?passing=true 2018-10-18 一、多服务的【事务】阻塞（跨机器） 数据库锁分为读锁、写锁，读读共享，写写互斥，读写互斥 程序A正在开启事务,操作(包括CRUD) 数据库记录A时,A会被行级锁（读/写锁）; 其它程序若要对进行互斥锁操作,需要阻塞到该锁被释放(程序A提交事务)， 2018-09-29 一、接口返回的JSON数据,快速转换为实际数据 ObjectMapper mapper = new ObjectMapper(); SimsPudo simsPudo = mapper.convertValue(responseMessage.getData(), SimsPudo.class); 2018-09-29 一、-XX:+PrintFlagsFinal :=意味着值是被修改的, =表示默认值 2018-09-29 一、Feign重试 默认只会对connect timeout进行重试 OKToRetryOnAllOperations=true 会对connect timeout和socket read timeout都进行重试,对socket read timeout会引起后端重复处理请求问题(需要做幂等) Feign对于&gt;400的后端报错是不会重试的 设置了OKToRetryOnAllOperations=true所有后端需要幂等 OKToRetryOnAllOperations=false的前端需要做对应的超时异常处理,如: i.写代码自动重试 ii.直接返回前台成功 二、超时时间 (socket)connect timeout 连接超时 (socket)read timeout 读超时 对read timout,请求已经到达后端处理,但是没在指定时间内返回 三、Http状态码分类 1XX:正在处理 2XX:请求处理成功 3XX:请求需要重定向 4XX:服务器无法处理请求(U Fuck Off) 5XX:服务器处理请求出错(I Fuck Off) 四、String “ABC”: 是显示声明的 以&quot;ABC&quot;形式存在于常量池中(常量池也在堆里) new String(“ABC”): 以对象形式存在于堆中 str.intern(),字符串(或引用)是否存在于常量池,不存在就把该引用存在常量池 “ABC”.intern() 没意思,本来就是放在常量池的东西,再调intern没用 五、@Transaction @Transactional方法会覆盖类上的配置 调用被注入的代理类才能有效地激活@Transaction的效果 2018-09-28 一、JVM参数配置 -XX:+PrintCommandLineFlags 打印改动过的JVM参数 -XX:+PrintFlagsFinal打印最终在用的参数 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions 显示隐藏参数 二、Feign前后端全局异常处理 后端【业务代码直接抛异常】 后端全局异常捕获时【返回带异常信息的ResponseMsg】(一般不含堆栈信息),同时返回状态码设置为500(也可以404,因为Feign默认后端报错就是返回404) 前端(调用者)ErrorDecode时,解析该[ResponseMsg的异常信息],重新throw对应的异常就能保证前后端异常一致了 [x] 对于需要进入fallback的调用 同上处理,但是按需可能需要使用FallbackFactory获取后端返回的异常信息进一步处理 如打印日志等 [ ] 问题:可能导致前端(调用方)不能切换实例重试 [ ] 加入Decode404=true后,404错误不会进入ErrorDecode和Fallback 2018-09-17 一、正确的kill进程 先kill -15(安全关闭 回收资源) 不行再kill -9(强制关闭) 2018-09-16 一、JDK8+移除了Perm jdk8移除了Perm 其方法区及常量池等数据,全部移到了元数据区(Metaspace)中 二、String.intern JDK7及以后版本,是复制其字符串引用到常量池中 实际数据还是存在于堆中 二、-XX:MetaspaceSize -XX:MetaspaceSize=200m不是初始元空间大小,而是达到了200m后才会对该区域进行GC 2018-09-06 一、获取全局唯一ID redis: 服务器时间戳+redis全局自增id=&gt;UUID 简单、快捷 zk:同上 比较慢 通过数据库 慢、并发低 twitter的雪花算法: 通过时间戳+机器ID=&gt;UUID 优势:速度快、无需依赖中间件、全局唯一 实现参考：https://github.com/souyunku/SnowFlake 2018-09-05 一、SQL的强制索引 select * from parcel FORCE INDEX(uniq_fpx_tracking_no_1) where fpx_tracking_no not in (‘901000486441’,‘901000497454’) ; 二、接口幂等理解 分布式锁实现幂等的方式 查询缓存结果,存在就返回 不存在,获取分布式锁(阻塞等待) 再尝试第一步(其实就是双重校验) 不存在,开始执行业务逻辑,并且缓存结果 释放锁 分布式锁实现的幂等,不完全可靠,因为缓存会过期 要保证其绝对可靠,还是得使用select+insert、唯一索引等方式 三、IO多路复用模型 https://mp.weixin.qq.com/s/xmSn9Xz6MiFb2s_0J7iXwQ 单Reactor单线程(Redis) 单Reactor多线程 多Reactor多线程(包括 Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，Netty 主从多线程模型的支持)","link":"/Make-A-Little-Progress-Every-Day/Make-A-Little-Progress-Every-Day.html"},{"title":"【Spring源码分析】循环依赖的处理","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/Spring/Spring-Circular-Dependencies.html 看源码的同学可以查看我的GitHub 上面在官方的基础上加入了大量中文注释，帮助理解 要了解的知识 什么是循环依赖 graph LR A-->B B-->C C-->A 存在哪些循环依赖 Setter循环依赖(可以被解决) 构造循环依赖(报错) 基于Prototype类型的循环依赖(报错) Bean的创建步骤 看源码的同学可以找到源码： 环节1~4的代码在AbstractAutowireCapableBeanFactory#doCreateBean方法中 环节5的代码在DefaultSingletonBeanRegistry#getSingleton(String,ObjectFactory)方法的addSingleton(beanName, singletonObject);中 Spring是怎么处理循环依赖的（对于单例Bean） 实现原理 Spring在创建单例BeanA的时候会先把BeanA(仅执行完构造方法)给放到三级缓存中， 当其他Bean或业务代码在BeanA[创建完之前]需要用到， 那么Spring就会把这个 还没进行[属性注入] [调用init方法]的BeanA提前暴露给这些Bean，并且把BeanA提到二级缓存中 三级缓存 首先咱们得知道三级缓存包括哪些： DefaultSingletonBeanRegistry#singletonObjects 单例对象的cache 只有[创建完成(只调用了构造方法)]&amp;&amp;[初始化属性完成]的才会放入这里 (这是一级缓存 我们平时说Spring是个大工厂，所有的创建好的bean都可以从Spring缓存里拿。 这里面说的缓存就是一级缓存) DefaultSingletonBeanRegistry#earlySingletonObjects 存放提前曝光的单例对象 只会放入[创建完成(只调用了构造方法)]&amp;&amp; [被提前曝光的] 的bean (用于解决[循环依赖]的2级缓存) DefaultSingletonBeanRegistry#singletonFactories 单例对象工厂的cache 只会放入[创建完成(只调用了构造方法)]的beanFactory (用于解决[循环依赖]的3级缓存) 解决循环依赖核心代码 123456789101112131415161718192021222324252627@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) { //desc 先从singletonObjects（一级缓存）取 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) { //desc 取不到且创建中,那么这里使用同步锁阻塞一会,等待创建完成 //❤这里会发现 当真正创建完bean时会调用addSingletonFactory() 这时候也会锁住singletonObjects❤ synchronized (this.singletonObjects) { //desc 同步阻塞+尝试从earlySingletonObjects(二级缓存)获取 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) { //desc 如果二级缓存取不到&amp;&amp;允许从singletonFactories通过getObject获取 //desc 通过singletonFactory.getObject()(三级缓存)获取工厂创建该bean ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { //desc 通过三级缓存的Factory创建目标bean 并放入2级缓存 singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject;} 其实这里做的事情和上面原理说的一样: 因为Spring在创建单例BeanA的时候会先把仅仅初始化完成,未注入属性的BeanA(对应图中步骤1)给放到三级缓存中， 如果其他Bean在BeanA创建完之前需要用到(循环依赖就是这种场景)， 那么Spring就会把这个BeanA提前暴露给这些Bean，并且把BeanA提到二级缓存中。 这样子，这些Bean就成功的获取到一个仅仅初始化完成,未注入属性的BeanA 场景分析 假设现在有3个Bean ABC发生了Setter循环依赖(如本文最上面的图) 通过如上方式,，Spring完全可以帮我们解决单例之间的Setter循环依赖问题让循环依赖的Bean之间获取到一个仅仅初始化完成,未注入属性的BeanA 这么说可能有些抽象，咱们尝试来还原该情况发生时发生了什么事情 现在getBeanA，会先去缓存里getA，这时候A还没被创建，故进行A的创建流程 因为A依赖于B，所以A会执行到createBean流程第三步的populateBean，然后会去getB(经过了第二步A已经被放入三级缓存中) 同理，B依赖C，所以B会执行到createBean流程第三步的populateBean，然后会去getC(经过了第二步B已经被放入三级缓存中) C依赖于A，也同理，但是这时候再去getA的时候，会发现又是一个getBeanA流程，但是这时候在三级缓存里getA已经能get到对象了 获取到这个缓存中的A之后，完成beanC的属性注入、初始化等操作。这样，就完成beanC的整个创建流程； 同理，B、A也一样： 这样逐级返回，这样，就完成了整个getBeanA的流程，虽然过程中存在循环依赖，但不会令getBean流程出现异常 至此，Spring完美地给我们处理了Setter循环依赖。 其实，换成代码理解可能更加简单:D 1234567A a = new A();B b = new B();C c = new C();a.setB(b);b.setC(c);c.setA(a); 其它一些补充 为什么构造循环依赖不能被解决（Bean创建过程中没有放入缓存） 参考createBean流程 根据上图的流程，createBeanInstance调用的实际上是构造方法，调用前还没把任何东西放入缓存中。 这时候如果出现基于构造方法的循环依赖，那么是不可能成功的，试想一下： newA的时候依赖于B newB的时候依赖于C newC的时候又反过来依赖于刚才的A 而这时候A还没被new出来！这时候异常就出现了 上述情况换成代码理解就成了 1A a = new A(new B(new C(new A(new ..........)))); 为什么prototype类型的循环依赖无法被解决 首先咱们得先了解下prototype的bean创建流程 graph TD A[createBeanInstance] C[populateBean] E[return Bean] A--调用Bean的构造方法得到一个空Bean-->C C--为Bean注入属性-->E 从流程可见，prototype的Bean创建过程中压根就没有对生成完的bean进行缓存 每一个Bean都是实时创建/使用的 根据上面的流程，我们很容易发现，这种不使用缓存的情况压根不允许存在循环依赖，因为： A依赖于B，这得实时去创建A和B B依赖于C，也得实时去创建C 创建C的时候发现C依赖于A，然鹅A没被缓存（当然，其实prototype压根不会去查缓存） 那这时候程序就会无限循环下去了(当然 spring会检测到这种异常,中断这次getBean) 所以，prototype类型的循环依赖是不能被解决、也不允许出现的 伪代码大致可以理解为： 1A a = new A(new B().setC(new C().setA(new A().setB(......)))); 总结 至此，循环依赖的讨论到此结束。其实，说白了就是： 只有[单例Bean之间]的的循环依赖才能被解决(因为只有单例Bean会被缓存到2级/3级缓存中 用以解决循环依赖) 只有[基于Setter属性注入]循环依赖才能被解决(因为只有基于Setter属性注入的Bean，才能在通过new调用构造方法后，把bean放入2级/3级缓存) [构造注入] 和 [prototype类型的bean]的循环依赖无法被解决 （因为没办法放入2级/3级缓存中）","link":"/Spring/Spring-Circular-Dependencies.html"},{"title":"架构已阅文档","text":"架构已阅文档 数据库相关 - 数据库索引，到底是什么做的 这篇文章还介绍了二叉搜索树,B树与B+树之间的区别 - 1分钟了解MyISAM与InnoDB的索引差异 - MySQL不为人知的主键与唯一索引约束 主要讲了MyISAM与InnoDB违反唯一索引时的场景,MyISAM会出现一个update语句，部分执行成功，部分执行失败(因为不支持事务) 分库分表 - 一分钟掌握数据库垂直拆分 - 炸！业界难题，跨库分页的几种常见方案","link":"/bookmarks/bookmarks.html"},{"title":"秒杀架构设计","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/framework-design/sec-kill-framework-design.html 常见的三类高并发场景 高并发压力主要来自,并发时出现大量锁冲突 1.细颗粒度操作-锁冲突少 如:QQ微信等即时通讯业务个人读个人自己的数据 数据结构 个人信息 user(uid…) 几十亿 个人的好友信息 friend(uid,friend_id…) 几百亿 个人的群 user_group(uid,group_id…) 几百亿 群成员 group_member(gid,uid…) 几千亿 个人消息(msg_id,uid…) 几千亿 群消息(msg_id,gid…) 几千亿 个人和群都是读写自己的数据 在高并发时(单个用户单位时间发出N个读写请求)，锁冲突极小，每个【人】、【群】、【消息】只会锁住自己部分的消息 在出现IO瓶颈的时候 只需要进行水平分库 把【人】、【群】、【消息】进行切分 2.读多写少,存在少量写冲突 如:微博 自己的写为别人的读 拉模式的大概数据结构 参考:微博Feed业务架构–推拉模式 个人信息 user (uid…) 几十亿 个人的关注列表 user_follow (id,uid,follow_id…) 几百亿 个人发出的微博 msg (msg_id,uid…) 几百亿 大概流程: 假设用的是拉模式，多个粉丝拉取别的某位用户的发件箱时容易出现读写锁冲突 如： 在某明星粉丝刷微博时,明星消息表、评论表被快速读写，出现锁冲突，宕机 3.(同一份数据)多读多写，存在大量写冲突 如：12306秒杀业务 大概数据结构 stock(s_id,time) //列车 ticket(t_id,num,s_id) //列车余票 用户量大,并发很大时, 有极大的锁冲突，极容易把系统搞垮 一辆车几百万请求，有效请求200，成功请求数0，最终请求成功率≈0% 解决【多读多写，存在大量写冲突】的锁冲突问题 主要方向为降低【数据库层面的锁冲突】 （1）降低读请求：利用缓存 （2）降低写请求：上游尽量过滤无效请求 （1）降低读请求：尽可能利用缓存 前端: 浏览器、Nginx等静态页面缓存 站点层、服务层: 缓存结果、缓存数据… （2）降低写请求：上游尽量过滤无效请求 前端: 通过JS做限速,减少99%请求(如:频繁点击,显示频率过快) 站点层: 拦截同个用户的重复请求(通过web层缓存或缓存集群 对[UID+TOKEN]进行计数&amp;限速) 服务层: 通过MQ、内存队列收集请求（队列长度根据数据库抗压能力、库存数量设置） 数据库层: 单个主从","link":"/framework-design/sec-kill-framework-design.html"},{"title":"记一次ES查询优化","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/ES-Query-Optimization.html 背景 难得有数据留存,写一篇^_^ 原逻辑 ES接收新单、改单、状态数据（占总数据的70%），写入ES的7天索引中 ES存在以天为单位的7个索引（如：20210101~20210107，7个索引） 存储逻辑： 根据订单创建时间，保存到对应月日的索引内，如：1月1日的保存到20210101 如果不再最近7天内的特殊订单,那么会存到今天最新的一天的索引内 查询查询: 接收到查单请求后，根据订单创建时间，根据月日指定去查询这7个索引的某一个或多个 如果是在最近7天的，那么保存到该日期对应的索引内 如果在最近7天以外的订单，那么会保存到今天最新的一天的索引内 更新逻辑: 先查询出原来的订单，然后更新，更新后保存到原来的索引内 初步分析 监控问题分析 大数据监控显示：在双十一前后ES监控IO读写次数过高，出现读/写拒绝，CPU占用高，初步分析主要原因有以下： 量大：写入ES的数据为新单、改单、状态变化3种，其中状态变化占据最多，共2亿/天的量 查询多：接收新单、改单、状态都需要查询一次ES 大部分查询无法使用索引：状态数据没有有效索引，会触发ES全局查询 初步分析人话结论 其实之中最主要的问题是因为接收状态数据的时候，没有创建时间字段，导致触发了ES全索引查询 状态数据量大，占大头，导致触发ES查询的量占超过70%，蛇打七寸，解决状态数据的查询问题，基本就可以解决问题了 场景论证 原逻辑场景分析 场景1 改单数据在7天外 操作状态数据无CreateTm字段，故7天内的单对应的状态与7天外的改单一致 有效查询率： 仅为1/35=2.85% 场景2 下改单数据为7天内的数据 有效查询率： 仅为1/5=20% 思路与方案 思路 其实经过场景论证后思路很明显，真正的问题所在便是接收新单、改单、状态数据的时候时间字段无法很有效地为ES查询指定分区，导致一个查询查了7个索引。 那么，问题解决思路就简单了： 为新单、改单、状态数据选择新的字段匹配ES索引，不再使用或者不再仅使用createTm字段 如1无法实现，ES换一个索引分区方式，如按照地区分区 很幸运的是，在思路1中，我们的内部订单号里包含着订单的创建时间，所以我们可以直接使用订单号来匹配ES索引，这样就可以很好地解决问题了。 优化方案 根据订单号规则，优化索引： 订单号00415022212022830806377748，其中0222为为订单生成日期 故直接以其为分区素引，直接查询、 更新该素引，根据场景不同至少减 少6/7的查询量 Other More 其实在上面演示的图解里还能发现，在每个索引里还存在5个分片，也许是因为当初大数据团队操作问题又或是什么原因没有把内部订单号作为ID， 导致通过内部订单号到ES查询订单信息的时候，ES会把这个订单号的所有分片都查询一遍，这也是导致ES查询量过大的一个原因。 额外优化 通过ES-API 指定查询路由/D，可以指定查询到唯一—个分片，减少80%的查询量 优化后 优化后逻辑 优化后逻辑(7天外）减少6/7(85.7%)查询量 优化后逻辑(7天内）减少4/5(80%)查询量 优化后效果(监控)","link":"/design/ES-Query-Optimization.html"},{"title":"Hystrix熔断优化","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/Hystrix-optimization.html 简述 本文说的其实就是 合理配置熔断，防止依赖的第三方接口响应过慢导致系统tomcat链接大量阻塞，最终导致系统崩溃的问题 顺便，将熔断配置从配置文件中提取出来，动态配置中心中，这样就可以通过动态配置中心来动态配置熔断参数了 除此，主要是团队里大家对单线程并发数与QPS概念有些混淆且计算方式了解不多，以及信号量与线程池方案选择上有些歧义，需要花了不少时间在会议上让团队达成一致。 背景&amp;分析问题 SISP客服系统、SISP查单系统等系统提供的服务都通过HTTP依赖于大量外部各种接口的响应， 这些接口的响应时间不可控，有时候会出现响应时间过长的情况，这时候如果不做任何处理，那么这些请求就会一直等待，这样就会导致系统的响应时间过长，甚至出现系统崩溃的情况。 双十一期间，SISP客服系统出现了系统崩溃问题，经排查发现tomcat线程池被耗尽，进一步排查是因为所依赖的PIS接口响应慢了 简单来说问题大概长这样： 其实在进入团队前，其实关键服务已经配置了相关的熔断，不过仔细看了下配置：execution.isolation.semaphore.maxConcurrentRequests=100000 这代表着需要同时有100000个线程进入该程序里才有可能触发熔断，在这种接口响应缓慢要死不死的情况下简直形同虚设 而hystrix错误率50%因为时间窗口太短+外部接口可用性也并不是在50%以下,难以触发,故难以触发熔断机制,导致系统崩溃; 以下是系统曾经的配置: 123456789101112131415161718@HystrixCommand( fallbackMethod = &quot;singleBack&quot;, commandProperties = { @HystrixProperty(name = &quot;execution.timeout.enabled&quot;, value = &quot;false&quot;), // 该属性用来设置在滚动时间窗中，断路器熔断的最小请求数。例如，默认该值为 20 的时候， // 当在配置时间窗口内达到此数量19的失败后，进行短路,默认20。 @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;20&quot;), // 该属性用来设置在滚动时间窗中，表示在滚动时间窗中，在请求数量超过 circuitBreaker.requestVolumeThreshold 的情况下， // 如果错误请求数的百分比超过50, 就把断路器设置为 &quot;打开&quot; 状态，否则就设置为 &quot;关闭&quot; 状态。 @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;50&quot;), // 滚动时间窗设置，该时间用于断路器判断健康度时需要收集信息的持续时间.默认：10000 @HystrixProperty(name = &quot;metrics.rollingStats.timeInMilliseconds&quot;, value = &quot;10000&quot;), // 该属性用来设置当断路器打开之后的休眠时间窗。 休眠时间窗结束之后，会将断路器置为 &quot;半开&quot; 状态，尝试熔断的请求命令， // 如果依然失败就将断路器继续设置为 &quot;打开&quot; 状态，如果成功就设置为 &quot;关闭&quot; 状态。默认：5000 @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;), @HystrixProperty(name = &quot;execution.isolation.strategy&quot;, value = &quot;SEMAPHORE&quot;), @HystrixProperty(name = &quot;execution.isolation.semaphore.maxConcurrentRequests&quot;, value = &quot;100000&quot;) }) 方案论述 看出问题就得出方案了，其实上面问题总结来说就2点： 1. 熔断配置在配置文件中，不方便动态调整（人看着都已经出问题了，还不能手动熔断快速恢复，领导都急疯了hhh） 2. 熔断配置maxConcurrentRequests不合理，导致熔断不起作用 maxConcurrentRequests配置 对于1.就不做过多叙述了，对于2.这里与团队成员有点分歧。。。 他们认为maxConcurrentRequests应该配置为接口的并发数。。。实际上，这个配置应该是单个节点最大并发数，而不是接口的并发数，这里花了我不少时间给团队成员解释这个问题。。。有点醉了。。。 这里简单说下，如果是单节点调用，那么maxConcurrentRequests就是接口的并发数，如果是多节点调用，那么maxConcurrentRequests就是单个节点的并发数 所以得出QPS和RT的关系： 对于单线程：QPS=1000/RT 对于多线程：QPS=1000*单节点并发线程数量/RT 对于多线程多接点：QPS=1000单节点并发线程数量节点数量/RT 因此，我们可以得出： 123maxConcurrentRequests（单节点线程数） = QPS /节点数/ ( 1000 / 被熔断方法的P99耗时ms )即:27000/128/(1000/20)=4.2QPS 所以得出,maxConcurrentRequests配置为5,理论上即可满足需求。 考虑到我们主要是为了解决单节点因为单个服务耗光tomcat容器所有http线程，这个值不需要设置得太严格，只要能保证单节点不会因为单个服务耗光tomcat容器所有http线程即可。所以，我们保守地设置为100 其实查阅官网，maxConcurrentRequests默认是10，并且说对于绝大部分、正常服务，一般来说都不需要修改这个值，他可以很好的满足绝大部分的场景，一定程度上说明我们上面求出的服务的maxConcurrentRequests=5也算是合理的 Other More 部门新来了架构师，在我在团队方案论述的说Hystrix就得用线程池，不要用信号量，他们以前公司就是这样用的，他和网上也推荐这样用，说可以异步、不占用tomcat线程什么的。 也许这个架构师不太理解我们的系统，又或者多Hystrix有什么误解，又或者过于相信网上的言论，当然，也可能是我个人理解有误，但是我觉得这个方案是不合理的，我这里说下我的理解： （信号量和线程池的区别这里就不多叙述了，自查吧 ） 因为我们系统有大量面向前端的服务，这些服务很多都依赖于其他第三方接口服务，如果每个都加上熔断，每个都设置自己的线程池，那么将会是一个恐怖的线程开销； 同时，我们99%的前端、后端业务调用这些服务，都需要这些服务有一个同步的响应，因此额外开线程池，释放tomcat线程就无从说起了，因此线程池的方式并不适合我们的场景。 因此当时否决回去了，但因团队成员坚持保守看法，所以后面又开了个会论述了一下这个事，终于如愿达成一致，最后还是采用了信号量的方式。 关键代码 动态配置关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 描述：熔断配置加载类，项目启动时会加载一次，然后修改动态配置中心的配置时会自动加载 * * &lt;pre&gt; * HISTORY * **************************************************************************** * ID DATE PERSON REASON * 1 2022/7/6 01390559 Create * **************************************************************************** * &lt;/pre&gt; * * @author Chris Cai * @version 1.0 */@Component@DisconfFile(filename = HystrixConfig.HYSTRIX_CONFIG_NAME)@DisconfUpdateService(classes = {HystrixConfig.class})public class HystrixConfig implements InitializingBean { /** * 熔断配置文件名 */ public static final String HYSTRIX_CONFIG_NAME = &quot;hystrix.properties&quot;; private static final Logger logger = LoggerFactory.getLogger(HystrixConfig.class); private static final int REFRESH_TIME = (int) TimeUnit.SECONDS.toMillis(30L); @Override public void afterPropertiesSet() { // 读取配置文件实现类 PolledConfigurationSource source = newConfigurationSource(); // 设置定时器，每隔30s检查一次读取一次配置文件 AbstractPollingScheduler scheduler = new FixedDelayPollingScheduler(REFRESH_TIME, REFRESH_TIME, false); // 创建动态配置类，并设置定时器和配置文件读取类，如配置发生变化，会更新对应的配置属性 DynamicConfiguration configuration = new DynamicConfiguration(source, scheduler); // 加入配置管理器 ConfigurationManager.install(configuration); } // 具体轮询业务逻辑 private PolledConfigurationSource newConfigurationSource() { return (initial, checkPoint) -&gt; { Properties properties = new Properties(); try (InputStream is = Thread.currentThread().getContextClassLoader().getResourceAsStream(HYSTRIX_CONFIG_NAME)) { properties.load(is); } catch (Exception e) { logger.error(&quot;fail load hystrix configs&quot;, e); throw e; } return PollResult.createFull((Map) properties); }; }} 熔断配置文件 各个服务通用default,各个服务可以单独配置，配置会覆盖default配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182### 全局默认配置 实例配置刷新清空时会临时取全局默认配置## 执行/隔离策略hystrix.command.default.execution.isolation.strategy=SEMAPHORE## 最大同时处理请求数hystrix.command.default.execution.isolation.semaphore.maxConcurrentRequests=100000hystrix.command.default.execution.timeout.enabled=false#熔断开关hystrix.command.default.circuitBreaker.enabled=true#熔断开关 如果为true 将会拒绝所有请求进入降级方法hystrix.command.default.circuitBreaker.forceOpen=false#熔断开关 如果为true 将会强制关闭熔断功能,无论错误百分比如何,它都会允许请求hystrix.command.default.circuitBreaker.forceClosed=false##该属性用来设置在滚动时间窗中,断路器熔断的最小请求数。例如,默认该值为 20 的时候,##当在配置时间窗口内达到此数量19的失败后,进行短路,默认20。hystrix.command.default.circuitBreaker.requestVolumeThreshold=20## 该属性用来设置在滚动时间窗中,表示在滚动时间窗中,在请求数量超过 circuitBreaker.requestVolumeThreshold 的情况下,## 如果错误请求数的百分比超过50, 就把断路器设置为 &quot;打开&quot; 状态,否则就设置为 &quot;关闭&quot; 状态。hystrix.command.default.circuitBreaker.errorThresholdPercentage=50##该属性用来设置当断路器打开之后的休眠时间窗。 休眠时间窗结束之后,会将断路器置为 &quot;半开&quot; 状态,尝试熔断的请求命令,##如果依然失败就将断路器继续设置为 &quot;打开&quot; 状态,如果成功就设置为 &quot;关闭&quot; 状态。默认:5000hystrix.command.default.circuitBreaker.sleepWindowInMilliseconds=5000##滚动时间窗设置,该时间用于断路器判断健康度时需要收集信息的持续时间.默认:10000毫秒.hystrix.command.default.metrics.rollingStats.timeInMilliseconds=10000### 实例配置 ###### 备注脱敏接口熔断配置## 最大同时处理请求数hystrix.command.remarkMask.execution.isolation.semaphore.maxConcurrentRequests=100### 运单查询接口熔断配置## 最大同时处理请求数hystrix.command.waybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 批量运单查询接口熔断配置## 最大同时处理请求数hystrix.command.batchWaybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 批量子单查询接口熔断配置## 最大同时处理请求数hystrix.command.batchChildWaybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 历史运单查询接口熔断配置## 最大同时处理请求数hystrix.command.hisWaybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 操作运单DSL查询接口熔断配置## 最大同时处理请求数hystrix.command.dslWaybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 综合查单接口熔断配置## 最大同时处理请求数hystrix.command.conditionsWaybillQuery.execution.isolation.semaphore.maxConcurrentRequests=100### PIS时效查询接口熔断配置## 最大同时处理请求数hystrix.command.pisTimeQuery.execution.isolation.semaphore.maxConcurrentRequests=100### 对内路由查询接口熔断配置## 最大同时处理请求数hystrix.command.insideRoute.execution.isolation.semaphore.maxConcurrentRequests=100### 对外路由查询接口熔断配置## 最大同时处理请求数hystrix.command.outsideRoute.execution.isolation.semaphore.maxConcurrentRequests=100","link":"/design/Hystrix-optimization.html"},{"title":"Kafka延时队列方案探讨","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/Kafka-Delay-Queue.html 目前在用方案:直接重新丢回队列后面 实现逻辑 引入延迟消息消费服务,消费延迟消息 每条消息消费时,Sleep3秒(很长),再处理; 处理时判断是否到点,没到点的数据丢回kafka 优点 不引入新依赖(不依赖DB,不依赖其他第三方) 缺点 1. 处理效率慢,并发低 2. 延时时间不精准,颗粒度非常大 3. 浪费Kafka空间,同一数据在Kafka多次存储(其实Kafka底层是一种文件/文档存储,消息的消费只读不删) 优化方案1: 延迟消息存DB,通过Redis的zset结构支持 ### 实现逻辑 #### 1. 发送延时消息: > 延时消息发送到延时队列TopicA ### 2. 消费延时消息: > 延时程序(消费者)消费延迟队列的消息,把延时消息存入DB,再把[发送时间]+[延时消息在DB记录ID]作为zset设到Redis ### 3. 监控&&发送[到时的消息]: > 1. 通过监控程序,监控Redis 发现[到时的任务],发送到真正的消费队列 进行真正的业务处理 > 2. 标识消息为已处理 或 删除(数据量大选立即删除,否则还是选择存几天,方便异常补数据) 缺点 强依赖Redis，数据存在: Redis出现异常会或会出现1s的数据丢失,补偿方案实现麻烦、效果不好(1.人工发现,人工通过日志/DB补偿 2.程序定时比对补偿) 会出现数据倾斜: 单个队列数据量大时,Redis会出现数据倾斜,导致Redis单点数据量大,更容易出现热点数据&amp;单点容量不足的问题 (这一点可以通过key,优化解决,就是每次存的时候麻烦点) 优化方案2: 分级队列+sleep方案 [分级延迟后面(5分钟 10分钟 30分钟…)] 实现逻辑 1. 根据需求,丢到对应的延迟主题中,如5分钟延迟主题 2. 五分钟主题按顺序消费前面的一条or多条数据时,判断该消息是否到达延迟时间,没到时间就sleep对应的时间; 到时间就消费掉&amp;&amp;发到业务主题中 (PS.得带上当前延迟主题分级DelayLevel，延迟次数tryCount，以方便业务主题消费失败时根据需求进行增量延迟操作) 3. 业务主题消费者消费时,根据[重试次数tryCount]&amp;[延迟主题分级DelayLevel]以及消费情况 决定正常消费掉消息 还是 丢到之前或别的分级延迟主题中； 缺点 1. Sleep太久，kafka会认为该消费者不可用，然后把消息给到别的消费者。。。 优化方案3: 分级队列方案+原生wait方案 缺点 1. 文档难找。。。没找到wait接口怎么用。。。只找到特性","link":"/design/Kafka-Delay-Queue.html"},{"title":"通用kafka延迟队列生产实践","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/general-kafka-delay-queue.html 接: Kafka延时队列方案探讨","link":"/design/general-kafka-delay-queue.html"},{"title":"Redis压缩方案设计--各种压缩方案单机的比较(CPU)","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/Redis-compress-design.html 1、 CPU性能对比（通过对比CPU时间） 对比论证： 选择序列化CPU使用率最低，约原来的16%~73%，其中FST序列化方式为原来的16%，速度最快 选择纯字符串压缩的方式因为是在原toJSON和parseObject的逻辑之间,再加入压缩逻辑,故性能比原生JSON会略差一些 （对比序列化&amp;压缩方案）每轮十万次测试 压缩方案单轮测试耗时最高，为0.42s，约每次使用该方案消耗0.04ms，对比原方案单次多次0.01ms的CPU时间，故认为以上方案对性能的损耗问题可忽略 序列化方案单论测试耗时最低，为0.05s，约每次使用该方案可节约0.027ms的CPU时间,对比压缩方案,可节约0.037ms的CPU时间，故认为序列化对性能提升不高 总结 纯序列化方案占用CPU时间少，纯压缩方案占用CPU时间稍多，但是都对整体性能影响极小 2、使用序列化实践的方案（速度快，压缩度高） 2.0 简要 核心参考资料 FST @Version官方文档 序列化总结 背景 发送给下游与Redis存储的对象为同一个对象 原理 对象通过直接序列化转byte，考虑到对象后续需要新增字段，可通过添加注解的形式进行扩展（必须给每个字段都加上唯一注解,包括原有字段） 2.1 FST方案设计(只针对大对象) 方案1描述 直接在需要存入Redis的对象中为每个新增字段加入新注解,新字段顺序有要求得放在其他字段之后 在自己相关工程，升级下游依赖到的相关包的版本号（需要梳理） 存取Redis时，进行序列化反序列化操作 方案1优点 一劳永逸,改动小 方案1缺陷 下游引入新的额外无用依赖 需要梳理自己项目，有哪些jar包给下游依赖到了，得逐个进行升级（容易疏漏？） 存到Redis的对象,如果想阉割属性,比较困难(比较简单有效的方案还是得新建类) 方案2描述 copy一个和原来的类(称为A类)一致的类(称为A’类)，在A’类上添加注解 保存redis前，通过属性copy，把A对象Copy到A’对象中，最终序列化保存A’对象 读取Redis时，同理把反序列化读到的A’对象,Copy给A对象 方案2优点 容易存储Redis的对象阉割字段(如:我们对于对于A类里有个大属性:jsonObject，我们不想存，那么只需要在A’上去掉这个字段) 下游无感知 方案2缺陷 对于每个需要扩展字段的对象对应的类，都得存在2份，可能会导致新建了非常多的类（如订单里引用的所有类型 都可能需要被扩展。。。） 每次存取都需要进行copy操作 在添加字段时，容易把A’类给忘了 序列化方案其他缺陷(核心难点) 被序列化的类,其涉及到的相关所有类,都得注册到serializer中；且序列化反序列化得用同一个serializer 改造较为复杂容易出错，需要嵌入到目前项目的RedisCache工具类中 少注册类时，反序列化会出错，且难以排查 切换新方案后,以前的key无法匹配到了,需要重新cache一次 2.2、Snappy方案设计（速度比原来略慢约增加，压缩度较高可减少约50%(实际存到redis可减少到75%左右)）(只针对大对象) 方案描述(Snappy压缩字符串) 存Redis前，先通过JsonUtil.toJsonString得到Json字符串后，然后对json进行Snappy压缩后，再存入Redis； 取Redis时，先通过Snappy解压，再通过JsonUtil.parseObject转回对象 方案优点 简单，说白了其实就是存取加入了进行压缩解压操作 下游无感知 对老数据可做兼容,业务开发时字段增减不会影响 Redis&lt;–&gt;对象 的读写与转换 方案缺点 增加的压缩逻辑导致耗时在原来基础上增加30%左右的CPU时间，但10w次读写也就+1s的处理时间，影响","link":"/design/Redis-compress-design.html"},{"title":"【Hbase优化_2】Hbase存储优化：Hbase数据压缩","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/hbase-data-compress.html 背景 接: 【Hbase优化_1】HBase 行键设计优化：解决数据倾斜问题 随着业务扩张，Hbase使用率越来越高，大数据那边反馈已经没有资源了，全公司都没有了，要扩资源得等公司出去采购，可以的话问我们能否看能不能优化一下Hbase存储，减少一下空间占用。 基于SISP巴枪数据使用业务场景，每行数据是整体从HBase中查询出来，不存在通过字段过滤查询数据、以及只查询某些字段的使用场景，都是将整条数据拿出来、或者整个运单的所有巴枪数据拿出来解析 方案 在可以放弃根据字段过滤的前提下，可以整行巴枪数据序列化存储到HBase。在查询数据时，再将数据做反序列化。序列化可以大幅减少空间占用。以下数据基于本地测试，使用 protobuf 方式进行序列化. 同时，对于历史数据集群，因为其存储为多列数据，因为不需要根据字段过滤，把字段合并为一列，减少列数，也进一步减少空间占用。 预计效果 基于当前实时和历史两个HBASE集群占用的空间大小，可以大致计算出序列化后所需要的 region 数量 （ 超过10g就算大的region 查询会慢些，region超过20g会自动分裂 。每台机器region不要超过500，超过500就会影响性能。 ） 当前集群情况 集群 当前集群占用 当前机器数量 当前集群region数量 实时数据集群 83692GB 18台 20762 历史数据集群 527976GB 32台 24845 预计优化后集群情况 集群 当前集群占用 当前机器数量 当前集群region数量 实时数据集群 36825GB(原来的44%) 18台 20762 历史数据集群 58078GB(原来的11%) 32台 24845 切换方案 生产上分为实时库与历史库，分别存3+1个月与1年。因为实施方案一样，这里只介绍实时库： 上线后3+1个月内读取方式不变，写库时改为新方案双写，以新规则写入新库同时也以旧的规则写入旧库 等3+1个月后，读取时只读取新库，写库时只写入新库 以上两步做一个开关，一键切换 半年后，删除旧库 题外话 这里最后因为使用了数据冗余的兼容过度方案，实际上在过度期间还浪费了不少Hbase资源，好在理论优化幅度确实高，领导们能批hhhh~","link":"/design/hbase-data-compress.html"},{"title":"【Hbase优化_1】HBase 行键设计优化：解决数据倾斜问题","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/hbase-region-data-skew.html 这次分享/记录如何通过优化行键设计来解决 HBase 中的数据倾斜问题 在大数据场景下，我们的系统使用 HBase 存储了大量的巴枪信息。其中巴枪信息分布在两个表中，分别是巴枪索引表和巴枪主表。 生产中经常会出现Hbase超时问题，用户也经常反馈原始路由查询、巴枪扫描数据导出等功能查询时间过长、超时等问题。经过分析，发现这些问题的根本原因是 HBase 数据倾斜问题。 1. 巴枪信息表结构 1.1 巴枪索引表 表结构 rowKey: [3位0~499分区号][5位网点编码]|[4位巴枪码]|[操作14位时间字符串]|操作号 value: 巴枪主表的rowkey 示例 rowKey: 2700755W|0030|202204060000129|031801088694 1.2 巴枪主表 表结构 rowKey: [3位0~499分区号][12位运单号]|[4位巴枪码]|[14位时间字符串]|[操作号] value: 巴枪数据JSON字符串 示例 rowKey: 330SF1352340043135|0021|20220404213004|SF1352340043135 2. HBase 数据倾斜问题 由于运单号生成规则问题，运单号的前几位具有很高的相似度。HBase 在数据写入 Region 时，根据 rowKey 进行字符串排序。在 Region 达到阈值分裂后，会出现“数据倾斜”现象，即部分 Region 数据量很大，部分 Region 数据量很小。这会导致大量小 Region 出现，严重消耗 HBase 存储资源、查询资源，同时降低查询效率。 具体点就是: Hbase根据分区把数据写入Region后，其数据根据RowKey进行字符串排序，分裂的时候会根据字符串顺序，从大到小切分一半数据到两个Region内，如原Region数据为SF00000-SF99999，切分后可能就是SF00000-SF49999，SF50000-SF99999两个分区 因为运单号生成规则问题，前几位相似度很高，因此可见巴枪主表rowKey,除去分区号后，其前几位区分度很低，所以如果hbase Region达到阈值分裂之后，新数据写入会出现『数据倾斜』现象，一些Region很大，一些Region极小，长期写入数据后，会出现大量的小region 3. 优化方案 3.1 新的巴枪主表 rowKey 规则 新的 rowKey 结构 rowKey: [3位0~499分区号][运单逆序hash取模3位][先逆序运单号]|[4位巴枪码]|[14位时间字符串] 3.2 对比 方案 总 Region 数量 最大 Region 存储数 最小 Region 存储数 分区量 数据倾斜度 现有方案 5129 113277 1 多 大 新方案 2500 26461 25380 少 小 结论 新的 rowKey 设计方案可以大幅减少 Region 数量，数据倾斜度极小，从而有效解决数据倾斜问题。 3.3 生产切换方案 生产上分为实时库与历史库，分别存3+1个月与1年。因为实施方案一样，这里只介绍实时库： 上线后3+1个月内读取方式不变，写库时改为新方案双写，以新规则写入新库同时也以旧的规则写入旧库 等3+1个月后，读取时只读取新库，写库时只写入新库 以上两步做一个开关，一键切换 半年后，删除旧库 4. 总结 优化效果因为需要时间沉淀，所以暂无数据对比结果; 通过优化 HBase 行键设计，我们可以解决数据倾斜问题，提高查询效率，降低存储和查询资源消耗。 在实际应用中，根据具体场景和需求，我们需要灵活调整行键设计策略，以实现更高效的数据存储和查询 附录（自己看）","link":"/design/hbase-region-data-skew.html"},{"title":"snappy压缩redis方案可行性验证","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/snappy-compress-redis.html Snappy压缩方案 研发环境(redis单节点，两个哨兵) 1. 压缩率 压缩前的redis内存： String类型保存1万份不同key，相同value的运单内存为119.61-1.02=118MB： Byte[]类型保存1万份不同key，相同value的运单内存为31.74-1.02=30MB： 结论：改造后的压缩率提升(118-30)/118=74% 2. cpu对比 1. 单线程1万次写入redis String类型，基本维持在5%以下： Byte[]类型，基本维持在5%以下： 结论：cpu对比无明显变化 2. 单线程一万次读取对比： String类型： Byte[]类型： 结论：cpu对比无明显变化 3. 耗时对比 单线程1万次写入redis耗时 单线程1万次读取redi耗时 String类型 135677ms 130027ms byte[]类型 128804ms（提升5%） 124215ms(提升4.5%) 结论：读写均有略微提升 二、测试环境 redis2主2备(4G内存/分片) 1、压缩率 压缩前的redis内存： String类型保存1万次，两个master共新增118MB Byte[]类型保存一万次，两个master共新增30MB 结论：改造后的压缩率提升(118-30)/118=74%，与研发环境结论一致 2、CPU对比 写优化前84%： 写优化后41%： 读优化前23%： 读优化后18%： 三、压缩改造 改造前： 将操作运单对象转换成String类型的Json，保存redis 改造后： 风险点： 1、所有查询的地方都需改造和兼容(包括web应用)，否则读取错误 2、只能用自己的web应用查看redis 回退方案：将功能开关关闭","link":"/design/snappy-compress-redis.html"},{"title":"微博Feed流读扩散设计","text":"参考主要来自58沈剑← [知识整理]根据个人理解整理后分享，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/Weibo-feed-design.html 什么是Feed流 Feed流即持续更新并呈现给用户内容的信息流 , 对于微博.微信朋友圈等业务刷新的数据都为Feed流 每条微博 朋友圈 为一条Feed 关键动作,关键数据 关键动作 关注 , 取关 发布Feed(朋友圈or微博) 获取自己主页的Feed流 核心数据 关系数据 Feed数据 难点 自己的主页由他人的Feed流组成 如果大家都是读写同一条Feed,会出现较大的读写冲突 造成系统的主要瓶颈 获取Feed流解决方案 模式有2种类: 拉模式 推模式 拉取模式 大致的数据结构 用户关系 用户的关注关系 user_follow(id,uid,follow_id…) 用户的粉丝关系 user_fans(id,uid,fans_id) //之所以要分成正反表是为了大数据量高并发情况下 可以做到分库 用户的消息列表(Feed) 用户发出的消息 user_msg(msg_id,uid,create_tm…) 流程 消息发布流程 用户A发出消息,只需要在[user_msg]表里插入一条消息 关注&amp;取关流程 以用户A取消关注用户B为例: 在A的关注列表里 删除B的记录 在B的粉丝列表里 删除A的记录 这里关注表和粉丝表不在同个库 但是不需要用到分布式事务, 用最终一致性就可以保证业务需求了 获取用户A主页的Feed流流程 [拉列表]获取用户A的所有的关注列表followList [通过列表再拉列表]遍历followList,获取所有被关注人发的所有消息的集合msgList (当然 这里得limit一下) [汇总排序]对每个msgList进行汇总排序 (这里可以利用最大堆把2,3合在一起) 优点： 存储结构简单，数据存储量较小，关系数据与feed数据都只存一份 取消关注，发布feed的业务流程非常简单 存储结构，业务流程都比较容易理解，非常适合项目早期用户量、数据量、并发量不大时的快速实现 缺点： 拉取朋友圈feed流列表的业务流程非常复杂 有多次数据访问，并且要进行大量的内存计算，大量数据的网络传输，性能较低 在拉模式中，系统的瓶颈容易出现在“用户所发布feed列表”的读取上，而每个用户发布feed的频率其实是很低的; 此时，架构优化的核心是通过缓存降低数据存储磁盘IO","link":"/design/Weibo-feed-design.html"},{"title":"生产数据库扩库","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/design/database-scale.html 背景","link":"/design/database-scale.html"},{"title":"以前自己定义的一些研发规范","text":"1234567891011121314151617181920212223242526272829gds-parent 根目录 gds-wms-parent 仓库运营系统服务 gds-wms 仓库运营系统服务 src common 公共类 utils 工具类 命名以Util结尾的类 domain 业务对象 enums 枚举类 exception 放自定义异常类 mapper 如果用JPA操作数据库用repository 放命名以Repository结尾的类, 用mybaits操作数据库用mapper 放命名以Mapper结尾的类 remote 远程调用，放fegin调用接口 consumer 存放[调用]外部Feign服务的类 命名以Client结尾 fallback 存放consumer调用降级处理类 命名以FallbackFactory结尾 provider 存放对外提供的HTTP(Feign)类 命名以Remote结尾 service 放业务逻辑处理接口 命名以Service结尾的类 impl 放业务处理实现`类 命名以对应接口名+Impl结尾的类(后面同理) web 放controller 命名以Controller结尾的类 ... resources mapper 存放sql的xml文件 static 存放静态资源(js/css/img....) templates 存放页面模板 error 存放错误相关的页面 wms 存放业务相关的页面 gds-wms-api 仓库运营系统服务Api gds-wos-parent 仓库作业系统服务 gds-wos 仓库作业系统服务 gds-wos-api 仓库作业系统服务Api","link":"/code-rules/Code-Rules.html"},{"title":"接口篇","text":"以下为以前开发自己设定的一些规范，供以后参考 1. 返回类型 所有接口返回类型都为ResponseMsg 除了与外界交互的接口,不允许其它返回类型为ResponseMsg的方法 2. 对外接口请求路径规则 提供给app用的接口统一以[/app]开头 提供给外部系统调用的接口统一以[/api]开头 提供给页面的接口统一以[/page]开头 3. Controller层做的事情 组装/校验参数 仅调用 “1次” Service层服务 组装返回ResponseMsg返回给调用方 其它补充 不要在Controller层写任何数据库操作的逻辑！包括查询！ 所有业务操作都放在Service层！Controller层只用来做校验，以及组装返回值！ 请务必注意！务必！务必！","link":"/code-rules/interface-code-rules.html"},{"title":"异常处理与日志篇","text":"异常处理 异常处理不需要手动输出日志 - 全局异常处理会帮你做这件事 遇到的所有异常都包装成[业务异常]or[系统异常]后往上抛 业务异常(校验异常等) 对应类:BusinessRuntimeException 常用方法: 1. throw BusinessRuntimeException.buildBusyException(EnumCommomSysErrorCode.MQ_ERROR, “消费异常”, parm); 2. throw BusinessRuntimeException.buildBusyException(parm,EnumCommomSysErrorCode.MQ_ERROR,); 系统异常(404,MQ联不通等) 对应类:SystemRuntimeException 常用方法: 1. throw SystemRuntimeException.buildSysException(EnumCommomSysErrorCode.FILE_TYPE_NOT_SUPPORT, e, parm); 2. throw SystemRuntimeException.buildSysException(EnumCommomSysErrorCode.FILE_TYPE_NOT_SUPPORT, “文件类型不支持”,e, parm); 业务日志打印 对HTTP请求(Controller)进来参数,不需要打印(对于Dubbo/MQ等入参还是需要打印的) - 已经做了拦截器全局进行打印","link":"/code-rules/exception-and-log-code-rules.html"},{"title":"数据库篇","text":"脚本提交 统一使用Flyway进行统一的管理 svn://172.16.30.16:20044/G2G_DS/trunk/WMS/wms_db_script Dao操作相关 对数据库表更新/删除操作不能使用ID作为’第一’条件, 如 错误用法 1update parcel set a=&quot;value&quot; where id=123 正确用法 应用业务主键作为条件 1update parcel set a=&quot;value&quot; where fpxTrackingNo=&quot;fpx20190402&quot; 数据库查询不允许使用select *, 应使用select a,b,c","link":"/code-rules/db-code-rules.html"},{"title":"MQ篇","text":"MQ队列命名规范 业务线_队列的生产者项目名_消费的项目名称_[Q/R/X]_自定义标识 如 : GDS_WMS_WOS_Q_FORECAST 对应的Exchange名为:GDS_WMS_WOS_X_FORECAST 对应的Routing key名为:GDS_WMS_WOS_R_FORECAST ; 对应的死信队列名为:GDS_WMS_WOS_Q_FORECAST_DEAD 对应的死信Exchange名为:GDS_WMS_WOS_X_FORECAST_DEAD 对应的死信Routing key名为:GDS_WMS_WOS_R_FORECAST_DEAD 生产者队列的消息统一通过 [MQ的shovels插件] 转发到消费者队列 生产者不需要创建死信队列 消费者队列必须测试一下消息失败是否会进入对应的死信 这,很重要 MQ队列/Exchange 定义规范 [生产者端]需要定义队列+Exchange 并且建立队列和Exchange的绑定关系 队列需要定义: durable=true exclusive=false, autoDelete=false 队列创建使用rabbitAdmin.declareQueue(queue); 防止队列窜到别的VH中 Exchange 需要定义 durable=true autoDelete=false Exchange创建使用rabbitAdmin.declareExchange(exchange);防止队列窜到别的VH中 建立绑定关系 rabbitAdmin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(“GDS_WMS_WMS_R_TASK_ASYNC_CONSUME”)); 例子 1234567//定义队列Queue queue = new Queue(&quot;GDS_WMS_WMS_Q_TASK_ASYNC_CONSUME&quot;, true, false, false);DirectExchange exchange = new DirectExchange(&quot;GDS_WMS_WMS_X_TASK_ASYNC_CONSUME&quot;, true, false);rabbitAdmin.declareQueue(queue);rabbitAdmin.declareExchange(exchange);rabbitAdmin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(&quot;GDS_WMS_WMS_R_TASK_ASYNC_CONSUME&quot;)); [消费者端]需要定义队列+Exchange 死信队列+死信Exchange 并且建立队列和Exchange的绑定关系 队列/死信队列需要定义: durable=true exclusive=false, autoDelete=false 队列创建使用rabbitAdmin.declareQueue(queue); 防止队列窜到别的VH中 Exchange/死信Exchange 需要定义 durable=true autoDelete=false Exchange创建使用rabbitAdmin.declareExchange(exchange);防止队列窜到别的VH中 建立绑定关系 rabbitAdmin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(“GDS_WMS_WMS_R_TASK_ASYNC_CONSUME”)); 例子 123456789101112131415//定义队列Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(2);map.put(&quot;x-dead-letter-exchange&quot;, &quot;GDS_WMS_WMS_X_TASK_ASYNC_CONSUME_DEAD&quot;);map.put(&quot;x-dead-letter-routing-key&quot;, &quot;GDS_WMS_WMS_R_TASK_ASYNC_CONSUME_DEAD&quot;);Queue queue = new Queue(&quot;GDS_WMS_WMS_Q_TASK_ASYNC_CONSUME&quot;, true, false, false, map);DirectExchange exchange = new DirectExchange(&quot;GDS_WMS_WMS_X_TASK_ASYNC_CONSUME&quot;, true, false);rabbitAdmin.declareQueue(queue);rabbitAdmin.declareExchange(exchange);rabbitAdmin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(&quot;GDS_WMS_WMS_R_TASK_ASYNC_CONSUME&quot;));//定义预报的死信队列(若配置转发生产者不需要定义死信队列)Queue deadQueue = new Queue(&quot;GDS_WMS_WMS_Q_TASK_ASYNC_CONSUME_DEAD&quot;, true, false, false);DirectExchange deadExchange = new DirectExchange(&quot;GDS_WMS_WMS_X_TASK_ASYNC_CONSUME_DEAD&quot;, true, false);rabbitAdmin.declareQueue(deadQueue);rabbitAdmin.declareExchange(deadExchange);rabbitAdmin.declareBinding(BindingBuilder.bind(deadQueue).to(deadExchange).with(&quot;GDS_WMS_WMS_R_TASK_ASYNC_CONSUME_DEAD&quot;));","link":"/code-rules/MQ-code-rules.html"},{"title":"Redis篇","text":"redis缓存Key规范 Key前缀统一使用常量: ConstantsString.RedisConstant.REDIS_CACHE_PREFIX","link":"/code-rules/MQ-code-rules.html"},{"title":"单元测试篇","text":"关于测试类的规范 (暂定) 单元测试应该是不依赖于别的单元测试的 所有单元测试应该都得回滚，如果存在异步处理的情况，应尽可能把主线程与fork线程拆成2个测试类方法进行测试 每个测试类／测试方法应写上对应的名称@DisplayName 每个接口，都必须写一个正向测试方法 关于测试类的类名：测试类与被测试的类的路径需要一致，名字也需要对应，如： 123com.fpx.wms.service.impl.InstockServiceImpl↓对应↓com.fpx.wms.service.impl.InstockServiceImplTest 关于测试类的方法名： 方法名尽可能为成功的条件如shouldSuccessAfterPay()，而方法具体用来测试哪个场景的，我们已经使用了@ DisplayName来描述，无须担心 对于结果，需要适应assert断言输出与结果是否一致（这才能算是一个单元测试） 断言统一使用AssertJ框架，使用Assertions.assertThat()进行处理 可以参考\\gds-parent\\gds-wms-parent\\gds-wms\\src\\test\\java\\com\\fpx\\gds\\wms\\service\\exceptionhandle\\impl在SVN版本为2962时提交的代码为参考","link":"/code-rules/JUnit-code-rules.html"},{"title":"待办篇","text":"待办目录 [X] 基于Junit5的新测试用例规范 [ ] 基于新测试用例的demo [ ] MQ重复消费问题解决 [X] 根据请求ID追踪调用链所有日志 系统改造 [ ] 新建GDS公用工程,存放Wms与Wos公用代码(暂定)","link":"/code-rules/todo-code-rules.html"},{"title":"MySQL锁","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/database/innodb-record-level-locks.html 官方参考 MySQL锁 InnoDB锁机制是基于索引建立的 如果SQL语句中匹配不到索引,那么就会升级为表锁 记录锁 1234-- id 列为主键列或唯一索引列SELECT * FROM table WHERE id = 1 FOR UPDATE;或update table set age=2 WHERE id = 1; 通过唯一索引实现的记录锁,只会锁住当前记录(必须为=不然会退化为临键锁) 间隙锁 间隙锁只有在事务隔离级别 RR(可重复读) 中才会生效. 为非唯一索引组成(如class,age等) 1select student where age&gt;26 and age&lt;28 lock in share mode ; 使用间隙锁的条件 命中普通索引锁定； 使用多列唯一索引； 使用唯一索引命中多行记录 临键锁(Next-key Locks) 临键锁只有在事务隔离级别 RR(可重复读) 中才会生效 是记录锁与间隙锁的组合 可以是唯一索引,也可以是非唯一索引,对其都以间隙锁的形式进行锁定(以唯一索引匹配,并且只匹配到一条数据除外) 临键锁(Next-key Locks) 例子: tno(唯一索引) tname tsex tbirthday prof depart age(非唯一索引) 858 张旭 1 1969-03-12 讲师 电子工程系 25 857 张旭 女1 1969-03-12 讲师 电子工程系 25 856 张旭 男 1969-03-12 讲师 电子工程系 25 831 刘冰 女 1977-08-14 助教 电子工程系 29 825 王萍 女 1972-05-05 助教 计算机系 28 804 李诚 男 1958-12-02 副教授 计算机系 26 其中有唯一索引的临键为: (-∞,804] (804,825] (825,831] (831,856] (856,857] (857,858] (858,+∞] 其中有非唯一索引的临键为: (-∞,25] (25,26] (26,28] (28,29] (29,+∞] 非唯一索引临键锁验证 123-- session1select * from teacher WHERE age between 26 and 28 lock in share mode ; 这时候会锁定非唯一索引的临键 (25,29] 所以我们测试更新age=25成功 插入age=27阻塞 更新age=29阻塞 插入age=30成功即可验证 12345678910111213-- session2-- 更新age=25--&gt;成功update teacher set tsex='女1' WHERE age=25;-- 插入age=27阻塞insert into `test`.`teacher` ( `tno`, `tname`, `tsex`, `tbirthday`, `prof`, `depart`,`age`) values ( '740', '张旭1', '12', '1969-03-12 00:00:00', '讲师', '电子工程系',27);-- 更新age=29--&gt;阻塞update teacher set tsex='女1' WHERE age=29;-- 更新age=30--&gt;成功insert into `test`.`teacher` ( `tno`, `tname`, `tsex`, `tbirthday`, `prof`, `depart`,`age`) values ( '740', '张旭1', '12', '1969-03-12 00:00:00', '讲师', '电子工程系',30); 唯一索引临键锁验证 12-- session1select * from teacher WHERE tno between &quot;831&quot; and &quot;856&quot; lock in share mode ; 根据上面的sql,我们匹配到唯一索引临键锁为:(825,857] 所以我们测试更新tno=825--&gt;成功 更新tno=857阻塞 更新age=858成功即可验证 123456-- 更新tno=&quot;825&quot;--&gt;成功update teacher set tsex='女1' WHERE tno=&quot;825&quot;;-- 更新tno=&quot;857&quot;--&gt;阻塞update teacher set tsex='女1' WHERE tno=&quot;857&quot;;-- 更新tno=&quot;858&quot;--&gt;成功update teacher set tsex='女1' WHERE tno=&quot;858&quot;;","link":"/database/innodb-record-level-locks.html"},{"title":"使用mycat后注意要点","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/database/careful-when-use-mycat.html 忙死，简单贴个笔记截图吧 主要就是注意一下mycat分发sql给到对应1~n个数据库,其数据库都会执行这一条sql,然后再去mycat那里聚合结果,所以要注意一下sql的写法 一些笔记","link":"/database/careful-when-use-mycat.html"},{"title":"分库分表","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/database/sub_library_and_sub_table.html 一些笔记","link":"/database/sub_library_and_sub_table.html"},{"title":"1分钟教你redis集群搭建（2服務器）","text":"遇到有疑惑的可参考这个链接 注意: 关闭redis集群不能直接kill掉进程，或者关机，我们要通过命令redis-cli -p 7001 shutdown进行关闭，这样在关闭之前，数据才能够进行保存 1. 安装 redis 详：略 2. 创建 n 个 redis.conf 文件(redis 集群需要至少6个节点[3主3从]) 详： 1234567daemonize yes //redis后台运行pidfile /var/run/redis_7000.pid //pidfile文件对应7000,7002,7003port 7000 //端口7000,7002,7003cluster-enabled yes //开启集群 把注释#去掉cluster-config-file nodes_7000.conf //集群的配置 配置文件首次启动自动生成 7000,7001,7002cluster-node-timeout 5000 //请求超时 设置5秒够了appendonly yes //aof日志开启 有需要就开启，它会每次写操作都记录一条日志 3. 用redis.server 启动 redis 12345678101服務器redis-server ./7000/redis.confredis-server ./7001/redis.confredis-server ./7002/redis.conf102服務器redis-server ./7003/redis.confredis-server ./7004/redis.confredis-server ./7005/redis.conf 4. 安装 redis (不安装也行 主要是为了安装redis-trib.rb脚本而已) 1gem install redis（需要ruby rubygems） 5. 关联集群 1redis-trib.rb create --replicas 1 172.16.86.101:7000 172.16.86.101:7001 172.16.86.101:7002 172.16.86.102:7003 172.16.86.102:7004 172.16.86.102:7005 6. 检查集群是否搭建完成 12345678910111213[chris-cai@localhost cluster]$ redis-cli -c -p 7000127.0.0.1:7000&gt; CLUSTER INFOcluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_sent:228cluster_stats_messages_received:228 其他说明 1. redis常用命令放到Linux系统目录中，方便直接使用 这3个文件可以复制到/usr/local/bin 中，日后可以直接在linux命令行调用，无需切换目录输入路径等 123redis-cliredis-serverredis-trib.rb 2. 常用命令 1234redis-cli -h 172.16.86.102 -p 7003redis-cli -c -p 7003cluster nodesCLUSTER INFO 让0b00721a509444db793d28448d8f02168b94bd38成为7000的从节点 1redis-cli -c -p 7000 cluster replicate 0b00721a509444db793d28448d8f02168b94bd38","link":"/redis/Redis-Install-Settring.html"},{"title":"Redis集群--sentinel 与 cluster的区别","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/redis/Different-Between-Sentinel-And-Cluster.html 一、sentinel 与 cluster的区别 Sentinel的作用 监控+自动故障迁移(自动升主) 定期监控redis是否按照预期良好地运行; 当一个master节点不可用时，能够选举出master的多个slave 并令其自动升主 PS. sentinel本身支持集群 Cluster的作用 分布式集群 对Redis进行16384个槽按照节点分片(默认为均分16384个槽到每个节点) 主从复制 监控+自动故障迁移(同Sentinel) 总结 Cluster 包含Sentinel的功能 Sentinel主要用于： 不需要分片的情况 监控+自动升主进程不想与Redis服务器部署在一起的情况 Redis3.x以下","link":"/redis/Different-Between-Sentinel-And-Cluster.html"},{"title":"Redis分片方案概要(Cluster or Codis)","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/redis/Redis-Partitioning.html Redis分片方案概要(Cluster or Codis) 客户端分片 常见的主要是Memcached 通过客户端Hash等方式决定数据要存到哪个节点 服务器端分片 Codis分片方案 Codis是一整套缓存解决方案，包含高可用、数据分片、监控、管理、动态扩态 etc. 走的是 Client-&gt;代理-&gt;redis，一定规模后基本都采用这种方式 限制 批量操作可能受影响: 不支持pipeline/watch/scan等批操作 不支持事务 支持mset/mget 官方Redis Cluster分片方案 走的是Client-&gt;redis server jump redis server 限制 批量操作可能受影响: mset/mget/pipeline/watch/scan等批操作需要所有key都存在以同个节点上 并且手动分片期间,批操作不可用 (参考官方文档的Implemented subset) 客户端必须支持集群协议 不支持事务","link":"/redis/Redis-Partitioning.html"},{"title":"Kafka整理","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/kafka/kafka-remark.html kafka为什么快 1. 通过生产和缓冲区减少网络开销 生产者发送消息时，会将多条消息打包为一个batch（发送缓冲区）一起发送，等缓冲区大小达到阈值或者一定时间，批量发送 2. 根据不同ack配置，可以不刷盘、少刷盘就响应ack ack=0 生产者发送消息后，不会等待Broker的响应，不保证消息是否到达Broker ack=1 生产者发送消息后，等待Leader Broker接收到消息后，返回ACK响应。 acks=all或acks=-1 生产者发送消息后，等待Leader Broker接收到消息，并且等待其他所有副本都成功复制消息（落盘），才会返回ACK响应。 注 节点接收到数据后不会立马刷盘，会先暂存到pagecache里，等到一定的大小或者时间后才会刷盘 3. 零拷贝 正常IO需要经过5次读写才能从磁盘读取数据发送给消费者 零拷贝可以实现内核态之间硬件的数据拷贝，只需要3次IO，不需要经过应用态，减少了2次非必要IO kafka实现延迟队列 1. 分级主题循环等待 正常生产，消费时如果时间不到，丢回队尾 一般采用相同（如10分钟）级别的delayed-messages，以免前面的消息堵塞后面的数据 2. 生产者方延迟发送消息 生产者生产延迟消息丢到第三方存储 如Redis/RocksDB中，再启动异步线程任务将过期消息丢到目标主题完成延迟消息的消费 顺序消费 1. 单个分区+单个消费者 问题：慢 2. 多个分区+多个消费者 借助partitioner，把有先后顺序的同个业务单分到同个分区消费 需要注意的问题 阻塞问题 消息处理失败最好不要在消费时候重试或者等待，以免阻塞后面的数据 如：消息消费时，先查一下重试表有没有这个单号的数据，没有就正常消费，否则保存到重试表让重试逻辑自己处理 处理积压数据 各个partition数据分布不均 各个partition数据分布不均，个别因为数据分区规则导致个别partition数据量很大，而一些又很小 优化分区规则，如：把分区号由城市改为订单号 消息体过大、非必要消息量过多，导致IO问题（kfk2次网络IO，2次磁盘IO） 优化消息体与减少消息量 因为促销等业务高峰原因 1. 消费时改为多线程消费 2. 改代码: 消费并发布到新主题，新主题的分区数为原来的3倍 原业务代码开3倍的消费者消费新的主题 慢，麻烦，有风险，需要新机器*2 3. 消费者用高性能机器替换 这里以后最好可以做好风险预案，做动态线程池 保证消费数据不丢失 1. 生产者 ack!=0，确保至少落到Leader Borker的磁盘后才给生产者ack 2. 消费者 保证至少是At-least-once消费模式 1. at most onece模式 禁用自动提交偏移量 接收到消息立马ack，然后再处理 2. at least once 禁用自动提交偏移量 接收到消息先处理，再提交偏移量 3. exactly once 禁用自动提交偏移量 处理消息 &amp; 提交偏移量 &amp; 保存消息id信息到第三方存储 注: 需要在同一个分布式事务管理器 补数 配置上(重置偏移量) 消费者配置中，KafkaConsumer 类的 setProperty 方法设置『auto.offset.reset』配置： earliest，重头消费 lastest，从当前消费（默认） none，当前消费者组没有偏移量，报错 业务上 重置偏移量 上游重发 将需要补数的数据暂存到其他新主题，或外部存储，后续用特殊新逻辑重新消费 自动提交偏移量(enable.auto.commit) 默认打开 在默认情况下，消费者会在每隔 5 秒钟的时间内将最近一次已消费消息的偏移量提交给 Kafka 服务器 auto.commit.interval.ms 如何保证高可用 多副本数据冗余，保证数据『高可用』读写 ISR（In-Sync Replicas）机制，只有同步了Leader的副本才可以参与读写，保证了数据的『一致性』 zk保证broker的『可用性』，当broker上下线、宕机，可以实现『分区自平衡』 持久化机制 leader机制，kafka一个分区有一个leader+多个follower副本，leader负责读写，故障会选举出follower作为新的leader","link":"/kafka/kafka-remark.html"},{"title":"HashMap-JDK1.8","text":"参考链接：HashMap在JDK1.8之前和之后的区别 JDK1.8之前 new HashMap(n)中的n为其容量 元素插入使用头插法 并发插入（resize时）会产生循环链表，在get一个不存在的元素时会导致死循环。参考：Java HashMap的死循环 JDK1.8之后 元素使用尾插法 new HashMap(n)中的n最接近的2^m为其容量 并发插入还是有问题，但不会产生死循环 插入时数组长度&gt;64，桶元素&gt;8时，会树型化 发生resize时，resize后，桶元素个数&lt;=6的，都会被解树型化 取模用与操作(hash &amp; (arrayLength-1))会比较快，所以数组的大小永远是2的N次方。你随便给一个初始值比如17会转为32 resize实现逻辑参考：Java HashMap工作原理及实现 HashMap关于static final int UNTREEIFY_THRESHOLD = 6的分析 遍历整个原始桶，把桶内数据分配到原桶与新桶中 具体图解分析可以参考以下链接的第六点：Java HashMap工作原理及实现 判断新桶元素个数&lt;=UNTREEIFY_THRESHOLD(也就是6)，那么解开树型化 所以，结论来说：只要是resize后，桶元素个数&lt;=6的，都是链表 indexFor为什么要用h&amp;(h-1) 位运算快 比方说h=16，不减1，那么算出来的结果全是0或16，这样会出现中间的槽完全不存东西 什么时候扩容 当前大小&gt;当前容量*扩展因子0.75，且计算出来的下标的卡槽不为空。","link":"/java/HashMap-JDK1.8.html"},{"title":"IO模型","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/system/IO-model.html 背景 服务器网络模型 这篇IO模型是《每天进步一点点》里的第一篇学习记录，真的是忘了看看了忘了。。其实不用记服务器的，大概理一下JAVA自己的就简单多了，这里稍微总结一下： JAVA IO模型 JAVA的IO模型分为BIO、NIO、AIO三种，其中BIO是阻塞IO，NIO是非阻塞IO，AIO是异步IO 阻塞IO模型： Java中常见的阻塞IO操作包括InputStream的read()方法和OutputStream的write()方法。这些操作在底层系统中对应着同步阻塞IO模型，即当应用程序调用这些操作时，如果数据没有准备好，操作将会一直阻塞，直到数据准备就绪才会返回结果。 非阻塞IO模型： Java中的非阻塞IO操作包括SocketChannel的read()和write()方法。这些操作在底层系统中对应着同步非阻塞IO模型，即当应用程序调用这些操作时，如果数据没有准备好，操作会立即返回并告诉应用程序数据是否准备就绪，如果数据没有准备好，应用程序可以继续执行其他任务而不是等待IO操作完成。 异步IO模型： Java中常见的异步IO操作包括AsynchronousSocketChannel的read()和write()方法。这些操作在底层系统中对应着异步IO模型，即当应用程序调用这些操作时，操作的完成会通过回调函数的方式通知应用程序，应用程序可以继续执行其他任务而不需要等待IO操作完成。 NIO与AIO的异同 其中非阻塞IO和异步IO看着很相似，其实说白了就是异步IO多了个回调，用户线程完全不需要等待（不用跟NIO那样一直在epoll那里等待） 一些笔记","link":"/system/IO-model.html"},{"title":"jstack定位线上问题","text":"jstack定位问题 出现死循环后，我们可以使用jstack命令来定位问题。 1. 查找JVM进程ID（pid=23199） 1`ps -ef | grep java` 2. 查找发生死循环的线程ID（threadId=23214），CPU利用率暴表的第一条记录就是了。将十进制23214转为十六进制5aae。 1`top -H -p 23199` 3. 使用jstack导出线程dump信息。 1`jstack -l 23199 &gt; ~/threaddump.txt` 4. 分析dump数据，查找CPU最高的线程的运行堆栈。可以看出，死循环发生在HashMap的put()方法。 1`cat threaddump.txt | grep -A10 5aae` 附：查看线程ID方法，windows工具下载地址 最后在Windows下揪出Java程序占用CPU很高的线程并找到问题代码，死循环线程代码","link":"/system/jstack-ding-wei-xian-shang-wen-ti.html"},{"title":"跨域请求减少一半请求","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/web/Reduce-half-of-the-cross-domain-requests.html 简述 点很小，作用挺大，简单写写 客服系统重构后，前端对应的后端服务域名不在同一个（这里没有用nginx解决跨域问题），每次请求都会发送两次请求，一次是预检请求，一次是正式请求。 这样就会导致请求的次数增加一倍，这样就会影响到用户的体验，尤其是对网络不好、国外的用户访问体验带来沉重的打机，新版客服系统上线后叫苦连连。 解决方案 这个问题困扰了好些日子，前些天突然查到一个配置可以很很简单快捷的解决这个问题，这个配置是Access-Control-Max-Age 原理： 后端返回的Access-Control-Max-Age 大于浏览器支持的最大值 那么取浏览器最大值作为缓存时间 否则取后端返回的Access-Control-Max-Age作为缓存时间 缓存时间内不会再发option请求 点这源码 配置 后端对CorsConfiguration配置Access-Control-Max-Age，前端请求时接收到Access-Control-Max-Age，在该有效时间内不会再发出Option请求 CorsConfiguration config = new CorsConfiguration(); config.setMaxAge(3600L); 我们粗暴地设置了1个小时，一个小时内，相同URL请求不需要再发两个同步请求，减少了一半请求量，大幅提高了用户体验 吐槽 其实我一直吐槽用nginx反向代理解决跨域问题就好了，但是架构已定，大动还得经过领导层层审批，且基本不可能通过，所以在跨域问题上花费大量时间。。。","link":"/web/Reduce-half-of-the-cross-domain-requests.html"},{"title":"java虚拟机栈的内存结构","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/JVM/JVM-Memory-Structure-Stack.html 学习JVM的可以去我的GitHub 上查看我的Xmind详细笔记 对整本《深入理解JVM》都有详尽的笔记，帮助理解 一、 前言 Java栈分为两种： Java虚拟机栈： 描述java【方法】执行的【内存模型】 每个线程进入每个方法对应一个栈帧 本地方法栈(本文不做描述) 同虚拟机栈，区别在于：虚拟机栈服务于Java方法（字节码），本地方法栈服务于Native方法 本文主要讲Java虚拟机栈的内存结构 一图胜千言 二、Java虚拟机栈的组成： 1. 栈帧 每个方法对应一个栈帧，在线程运行到该方法时才创建，随着方法结束而销毁 2. 线程栈 栈帧内存在线程内存上进行分配，每条线程能为栈帧分配的总大小最大值为-Xss 为了方便，我们这里 [把这条线程对应的内存] 称为 [线程栈] 3. Java虚拟机栈 Java虚拟机栈是描述java【方法】执行的【内存模型】–&gt; 实际上它是 当前时刻[所有的线程栈]集合的统称 三、 Java虚拟机栈会出现的异常: 经过上面的描述，我们可以清晰地知道： 【Java虚拟机栈】由【当前所有的线程栈】组成 【每个线程栈由】由【这条线程所对应的所有栈帧】组成 因此，可能会出现2种内存溢出的情况 （也可参考Oracle JavaSE8的JVM规范：The Structure of the Java Virtual Machine-Java Virtual Machine Stacks） [x] 线程栈溢出(Stack Overflow)： 某条线程对应的栈帧内存之和 超过线程栈内存的最大值-Xss 根据栈帧的定义可以理解到：该问题主要是一条线程在某一时间点同时存在于多个方法中（网上称作方法深度过深，最常见的案例就是递归调用） [x] 从Java虚拟机栈层面溢出(OOM)： 当前时刻[所有的线程栈]内存之和 超过 Java虚拟机栈所允许的最大值 导致这个问题的原因是当前新建的线程数太多(当然,换句话也以说是当前计算机可分配给Java虚拟机的空间太少) 每条线程占用的空间都未-Xss，新建多了也就OOM了；这种情况一般较少发生，主要有以下因素： 系统对线程数上限的限制 线程过多，会导致CPU崩溃，也就是说系统早就挂了 Java虚拟机栈所允许的最大值：JVM 可分配内存-其他JVM内存结构空间最大值(主要是堆)–&gt;剩下的作为栈的总空间最大值(来自《深入理解Java虚拟机》P54) 四、验证 4.1. 线程栈溢出（StackOverFlow） 通过-Xss调整线程栈大小测试[线程栈最大占用内存为-Xss] 1234567891011121314151617181920212223242526272829private static AtomicLong i = new AtomicLong(0);public static void main(String[] args) throws InterruptedException { //1. 测试错误StackOverFlow testStackOverFlow(); //2.测试错误OOM //testOOM();}/** * StackOverFlow每次深度不一样是因为JIT优化 * -Djava.compiler=NONE禁用JIT优化后每次深度一样 * * * 测试参数：-Xss256k*/private static void testStackOverFlow() { //new Thread(() -&gt; { StackErrorTest testData = new StackErrorTest(); try { testData.test1(); } catch (Throwable e) { System.out.println(Thread.currentThread().getName() + &quot; &quot; + i.get()); } //}).start();} 运行参数设置-Xss256k -Djava.compiler=NONE,多次运行输出结果一致: 1main 1882 运行参数设置-Xss512k -Djava.compiler=NONE,多次运行输出结果一致: 1main 4861 2.2.2 从Java虚拟机栈溢出（OOM） 注意 Mac有单进程线程数量限制,16G的Mac电脑限制为5000,修改方式参考(不好意思 不能修改 ) 在一些配置较低的电脑可能会死机，请小心运行 建议使用虚拟机运行 影响因素 描述 系统限制 系统允许进程的最大线程数 -Xss 每条线程栈占用的内存 -Xmx 堆空间最大值 详细验证可以参考这篇文章 ---","link":"/JVM/JVM-Memory-Structure-Stack.html"},{"title":"JVM内存结构-总纲","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/JVM/JVM-Memory-Structure-Menu.html 学习JVM的可以去我的GitHub 上查看我的Xmind详细笔记 对整本《深入理解JVM》都有详尽的笔记，帮助理解 前言 网上有不少描述JVM内存结构的文章，但是要么比较老久了，要么描述有误，今天根据自己的理解整理下，有误请指正。 整体图解 程序计数器 记录Java程序运行到哪里 线程私有，可以看做当前线程执行到哪行【字节码】 字节码解析器工作就是通过改变这个【计数器】来选择下一行要执行什么，分支、循环、线程恢复都依赖于它 若为Java方法，则记录当前执行的字节码指令地址； 若执行的是native方法，则为空 Java 虚拟机栈 描述java【方法】执行的【内存模型】 每个方法对应一个栈帧,在线程运行到该方法时才创建 一条线程拥有的栈帧之和最大为-Xss(我们这里把它叫做线程栈) 当前所有线程栈之和=当前Java虚拟机栈已用大小 Java虚拟机栈总空间最大值：JVM 可分配内存-堆最大值-其他空间最大值–&gt;剩下的作为栈的总空间最大值 详细可参考: java虚拟机栈的内存结构 栈帧结构(待填坑) Java虚拟机栈异常及其处理方案(待填坑) 本地方法栈 同虚拟机栈，区别在于：虚拟机栈服务于Java方法（字节码），本地方法栈服务于Native方法 堆 随JVM启动而创建，是虚拟机最大的一块内存，被所有线程共享 存放着对象与数组等一切new出来的对象 垃圾收集器主要管理的区域 还存放着常量池 (1.7及以后的版本,都移到堆里存储了,需要注意的地方比较多 后面会说 不在这里描述) 详细可参考我另外的这篇博客(待填坑) 字符串常量池 存放字符串常量池,不同JDK版本存放的内容不一样 **1. 怎样的String会被存到常量池 简单来说以下这些情况都会存入常量池: 直接使用双引号声明出来的String对象 调用intern()方法 这个可以参考这篇《深入解析String#intern》 何时存放String字符串也可以参考 String放入运行时常量池的时机与String.intern()方法解惑 2. 存储结构在不同JDK下的区别 JDK≤6 常量池存于方法区中 常量池里的内容全部为字符串具体的值 JDK≥7 常量池存于堆中来自官网原文:the string pool was relocated to the heap 存储的东西为字符串值或引用(引用堆里的值),具体可以参考美团的《深入解析String#intern》 3. String 加载进字符串常量池的方式/时机 首先要知道字符串常量池是位于运行时常量池中的 编译完刚启动: 加载String进字符串常量池的过程大致为： graph LR A[编译后的class文件中的class常量池] B[运行时常量池中的字符串常量池] A-->B; 运行期间 graph LR A[代码] B[运行时常量池中的字符串常量池] A-->B; 参考《常量池结构及其加载过程》 方法区(永久代) 存放类的结构：版本、常量、全局变量、静态变量、方法、接口、即JIT编译后的代码等信息（JDK8后完全移出方法区 可以参考:Java 8: From PermGen to Metaspace) 具体里面存了啥 可以参考 JVM虚拟机结构 (不在JVM里的)元数据区 JDK8开始,替代方法区的存在；很大程度的避免了因为类加载过多导致的\bOOM问题 实际上元数据区不属于JVM内存的一部分；其为本地内存的一部分；大小取决于\b开发人员配置或可用的本地内存大小 有一点必须注意的是： -XX:MetaspaceSize=128m 不是初始元空间大小,而是达到了128m后才会对该区域进行GC 初始化大小20.8m 默认MetaspaceSize也是20.8m","link":"/JVM/JVM-Memory-Structure-Menu.html"},{"title":"垃圾回收器选型","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/JVM/jvm-gc.html 总揽 吞吐量和最短停顿时间本来就互相矛盾 Parallel Old追求的是吞吐量，CMS追求的是STW的最短 而G1通过把堆分成多个相对独立的Region块，并行的进行选择性的回收，实现一个两者兼顾的回收器 Parallel GC： 适用于吞吐量优先的场景 原理: 通过参数-XX:GCTimeRatio 设置垃圾回收时间占总时间的比例，默认值为99，即垃圾回收时间不超过1%，实现吞吐量优先 CMS（Concurrent Mark Sweep）GC： 适用于响应时间优先的场景 原理1：通过-XX:CMSInitiatingOccupancyFraction预留空间实现一边回收垃圾，一边执行业务逻辑，实现响应时间优先 原理2：通过并发标记等，实现尽可能低的停顿 缺点1: 内存使用率低，为了一边干活一边回收垃圾，预留了一定的内存 缺点2：产生碎片，虽然FullGC时候，但是运行期间会产生大量碎片 （新增）G1（Garbage First）GC： 适用于大堆内存场景，通过Region分区,既实现超短GC暂停时间，同时保证吞吐量 原理1:通过Region分区+预测模型,优先回收垃圾最多的Region，实现可控制时间的超短GC暂停时间 原理2：使用复制算法，没有碎片 缺点1: 内存使用率低，因为新生代与老年代都用了复制算法，需要预留一定的内存 缺点2: GC效率低，每次都为不完全GC 缺点2: 默认内存使用45%开始FullGC （新增）什么时候使用G1 G1 的第一个重点是为运行需要大堆且 GC 延迟有限的应用程序的用户提供解决方案 非官方提示大约 6GB 或更大的堆大小，以及低于 0.5 秒的稳定且可预测的暂停时间选择G1 如果应用程序具有以下一个或多个特征，则现在使用 CMS 或 ParallelOldGC 垃圾收集器运行的应用程序将受益于切换到 G1。 Full GC 持续时间太长或太频繁。 对象分配率或提升率差异很大 不需要的长时间垃圾收集或压缩暂停（超过 0.5 到 1 秒） 现有系统垃圾回收器选型 admin（-Xmx8g -Xms8g）服务于前端，提供实时同步的http接口，注重高效响应，当前使用的是默认的Parallel GC，应该考虑使用CMS G1回收器，并限制最大停顿时间&lt;20 Cache（-Xmx4g -Xms4g）不单独提供服务，主要负责数据消费与写入，注重吞吐量，当前使用的是CMS，不妥，应考虑使用Parallel GC Report（-Xmx5g -Xms5g）,主要报表异步导出，注重吞吐量，当前使用的是G1，其实用Parallel GC可能也差不多 Order：(Xmx4G -Xms4G)，G1回收器，注重吞吐量，或许Parallel也是个很好的选择 Operation：(Xmx12G -Xms12G) 想说G1回收器没跑了，目前也是用的这个，但作为数据消费者与生产者其实基本不怎么GC，其业务只注重吞吐量，其实或许Parallel也是个很好的选择，但这是个坑，因为这可能导致超长停顿，让与中间件等服务器之间的连接超时，出现故障","link":"/JVM/jvm-gc.html"},{"title":"深入理解Java虚拟机-阅读整理","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/JVM/JVM-Xmind.html 看完《深入理解Java虚拟机》了(好像是教这个名字吧😂)，花了小几个月边看边整理，终于呕心沥血地整理完的，书籍作者基于jdk7写的，加入分析对比了一些jdk8的新特性，以及官方文献的引用，因为JVM还是比较底层灰色，难免有些地方可能有误，欢迎指正。 图片很大，需要放大看，实在看不清，可以去github上自取源Xmind文件:点这里跳转 （有一章类结构偷懒没看，自我感觉用处不是很大）","link":"/JVM/JVM-Xmind.html"},{"title":"报表JVM调优","text":"[原创]个人理解，请批判接受，有误请指正。转载请注明出处: https://heyfl.gitee.io/JVM/report-jvm-optimize.html 忘记记录了。。凭记忆记录下😂，下面是大概流程 背景 之前上线的一个新报表项目，我们的报表服务主要通过定时任务异步生成报表，报表比较大，每次运行时长长，实时性要求较低。 但在生产环境普罗米修斯监控中，发现服务频繁进行GC，且GC前后释放的内存不多，GC期间CPU占用略高。但是GC时长看着很短，短期问题不大，但是还是得看一下，为了解决这个问题，我进行了一次针对报表服务的JVM调优 过程 1. 看监控 使用Prometheus监控JVM指标，并结合Grafana进行数据可视化。通过Prometheus收集报表服务的JVM指标，如GC情况、内存使用、线程状态等 发现回收间隔时间较短，且回收前后释放的内存不是特别多，看了下GC配置，G1+默认200ms的回收间隔，估计是这个时间间隔搞个鬼，不太适合我们的业务场景（报表比较大，每次运行时长长）。 2. jstat确认GC前后内存变化 1jstat -gc 12345 1000 10 样例： S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 68224.0 68224.0 0.0 34080.7 272896.0 272896.0 68224.0 68224.0 68224.0 27327.0 34080.7 10258.0 20 0.269 2 0.044 0.313 68224.0 68224.0 0.0 34080.7 272896.0 272896.0 68224.0 68224.0 68224.0 27327.0 34080.7 10258.0 20 0.269 2 0.044 0.313 结果发现 每次老年代回收率较高，报表服务的对象不该存放在老年代，应该调整到新生代 GC时间较短，我们用的是G1回收器，所以认为其实每次都不能很好的回收足够的内存，导致频繁的GC 3. jmap确认内存占用 1jmap -dump:format=b,file=heapdump.hprof 12345 因为当前服务实时性与可用性要求不是特别高，直接打dump问题不大； 但如果是实时性要求较高的服务，可以使用jmap -histo:live 12345查看内存占用情况，然后再决定是否打dump： 12jmap -histo:live 12345 | sort -n -r -k 2 | head -10 # 类实例实例数前10jmap -histo:live 12345 | sort -n -r -k 3 | head -10 #类实例总大小前10 4. MAT分析dump 使用MAT（Memory Analyzer Tool）分析heap dump，找出内存中的大对象以及引用关系。 结合源码，发现在处理历史巴枪数据导出大任务报表时，单次查询数量没做限制，只有工具自带的最高阈值1w，用户根据网点查询就能达到阈值，然后单个任务写入报表也创建了大量的临时对象，在多个任务运行时很容易触发GC，GC后又不能有效释放这些内存，导致频繁GC。 5. 做出的调整 其实这个问题就是因为GC频繁，以及新生代对象过快晋升导致的；解决就相对简单了： 调整G1的暂停时间目标（-XX:MaxGCPauseMillis=500，默认200） 调整新生代晋升阈值（-XX:MaxTenuringThreshold=20，默认15） 调整业务限制，该巴枪扫描业务单次查询数量限制在5000内，以防单次处理量过大，消耗大量内存。 工具类调整，限制在5000，业务调用时按需扩大，以防再次出现类似问题 新生代与老年代稳妥起见的比例暂不调整，因为该服务还加在了大量本地缓存等，理论上老年代还是相对要比较大一点的 除此以外，其实把垃圾回收器换成Parallel GC可能会更好，但是考虑到系统迭代会有更多的报表任务需要处理，以后内存可能还要逐步加，对大内存来说，还是先继续用G1吧 6. 效果 通过以上调优措施，报表服务的老年代回收率得到了有效改善，频繁GC现象得到缓解，进而提升了服务的性能和稳定性。 附录：现有系统垃圾回收器选型 admin（-Xmx8g -Xms8g）服务于前端，提供实时同步的http接口，注重高效响应，当前使用的是默认的Parallel GC，应该考虑使用CMS G1回收器，并限制最大停顿时间&lt;20 Cache（-Xmx4g -Xms4g）不单独提供服务，主要负责数据消费与写入，注重吞吐量，当前使用的是CMS，不妥，应考虑使用Parallel GC Report（-Xmx5g -Xms5g）,主要报表异步导出，注重吞吐量，当前使用的是G1，其实用Parallel GC可能也差不多 Order：(Xmx4G -Xms4G)，G1回收器，注重吞吐量，或许Parallel也是个很好的选择 Operation：(Xmx12G -Xms12G) 想说G1回收器没跑了，目前也是用的这个，但作为数据消费者与生产者其实基本不怎么GC，其业务只注重吞吐量，其实或许Parallel也是个很好的选择，但这是个坑，因为这可能导致超长停顿，让与中间件等服务器之间的连接超时，出现故障","link":"/JVM/report-jvm-optimize.html"}],"tags":[{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"BUG","slug":"BUG","link":"/tags/BUG/"},{"name":"优化，JAVA","slug":"优化，JAVA","link":"/tags/%E4%BC%98%E5%8C%96%EF%BC%8CJAVA/"},{"name":"优化","slug":"优化","link":"/tags/%E4%BC%98%E5%8C%96/"},{"name":"Spring-Cloud","slug":"Spring-Cloud","link":"/tags/Spring-Cloud/"},{"name":"Consul","slug":"Consul","link":"/tags/Consul/"},{"name":"design","slug":"design","link":"/tags/design/"},{"name":"MYSQL","slug":"MYSQL","link":"/tags/MYSQL/"},{"name":"每天进步一点点","slug":"每天进步一点点","link":"/tags/%E6%AF%8F%E5%A4%A9%E8%BF%9B%E6%AD%A5%E4%B8%80%E7%82%B9%E7%82%B9/"},{"name":"知识点整理","slug":"知识点整理","link":"/tags/%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"架构设计","slug":"架构设计","link":"/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"高并发","slug":"高并发","link":"/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"Hystrix","slug":"Hystrix","link":"/tags/Hystrix/"},{"name":"KAFKA","slug":"KAFKA","link":"/tags/KAFKA/"},{"name":"REDIS","slug":"REDIS","link":"/tags/REDIS/"},{"name":"Hbase","slug":"Hbase","link":"/tags/Hbase/"},{"name":"MYCAT","slug":"MYCAT","link":"/tags/MYCAT/"},{"name":"Code-Rule","slug":"Code-Rule","link":"/tags/Code-Rule/"},{"name":"DATABASE","slug":"DATABASE","link":"/tags/DATABASE/"},{"name":"LOCK","slug":"LOCK","link":"/tags/LOCK/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"redis集群","slug":"redis集群","link":"/tags/redis%E9%9B%86%E7%BE%A4/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"JAVA","slug":"JAVA","link":"/tags/JAVA/"},{"name":"基础","slug":"基础","link":"/tags/%E5%9F%BA%E7%A1%80/"},{"name":"SYSTEM","slug":"SYSTEM","link":"/tags/SYSTEM/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"JVM内存结构","slug":"JVM内存结构","link":"/tags/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/"}],"categories":[{"name":"about","slug":"about","link":"/categories/about/"},{"name":"bookmarks","slug":"bookmarks","link":"/categories/bookmarks/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"JVM","slug":"JVM","link":"/categories/JVM/"},{"name":"Distributed","slug":"Distributed","link":"/categories/Distributed/"},{"name":"Bug-Log&amp;Optimization","slug":"Bug-Log-Optimization","link":"/categories/Bug-Log-Optimization/"},{"name":"Make-A-Little-Progress-Every-Day","slug":"Make-A-Little-Progress-Every-Day","link":"/categories/Make-A-Little-Progress-Every-Day/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"framework-design","slug":"framework-design","link":"/categories/framework-design/"},{"name":"design","slug":"design","link":"/categories/design/"},{"name":"code-rules","slug":"code-rules","link":"/categories/code-rules/"},{"name":"database","slug":"database","link":"/categories/database/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"system","slug":"system","link":"/categories/system/"},{"name":"web","slug":"web","link":"/categories/web/"}],"pages":[]}